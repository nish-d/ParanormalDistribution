{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import radians\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 20254\n",
      "Test size: 6966\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "print(f\"Train size: {df_train.shape[0]}\")\n",
    "print(f\"Test size: {df_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primary School"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pri_sch = pd.read_csv('../data/auxiliary-data/sg-primary-schools.csv')\n",
    "df_pri_sch = df_pri_sch.drop([\"name\", \"lat\", \"lng\", 'planning_area'], axis=1)\n",
    "pri_sch_cleaned = df_pri_sch.value_counts().to_frame(name=\"pri_sch\")\n",
    "\n",
    "#TODO: Get schools in vicinity instead of just subzone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondary School"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sec_sch = pd.read_csv('../data/auxiliary-data/sg-secondary-schools.csv')\n",
    "df_sec_sch = df_sec_sch.drop([\"name\", \"lat\", \"lng\", 'planning_area'], axis=1)\n",
    "sec_sch_cleaned = df_sec_sch.value_counts().to_frame(name=\"sec_sch\")\n",
    "\n",
    "#TODO: Get schools in vicinity instead of just subzone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Population Density of Subzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subzone = pd.read_csv('../data/auxiliary-data/sg-subzones.csv')\n",
    "df_subzone[\"population_density\"] = df_subzone['population']/df_subzone[\"area_size\"]\n",
    "df_subzone = df_subzone.drop(['area_size', 'population', 'planning_area'],axis=1).set_index(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dirty_test = df_test\n",
    "df_dirty_train = df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates and invalid data (Rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records dropped :3034\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = df_dirty_train.drop_duplicates()\n",
    "df_cleaned = df_cleaned[df_cleaned.size_sqft > 0]\n",
    "df_cleaned.dropna(subset=['num_beds', 'num_baths', 'price', 'size_sqft', 'built_year', 'available_unit_types', 'tenure'], inplace = True)\n",
    "df_cleaned = df_cleaned[df_cleaned.price > 0]\n",
    "df_cleaned = df_cleaned[df_cleaned.furnishing != \"na\"]\n",
    "print(f'Records dropped :{df_dirty_train.shape[0] - df_cleaned.shape[0]}' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#c.\tStandardize capitalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['property_type'] = df_cleaned['property_type'].str.lower()\n",
    "df_cleaned['tenure'] = df_cleaned['tenure'].str.lower()\n",
    "df_cleaned['furnishing'] = df_cleaned['furnishing'].str.lower()\n",
    "df_cleaned['subzone'] = df_cleaned['subzone'].str.lower()\n",
    "df_cleaned['planning_area'] = df_cleaned['planning_area'].str.lower()\n",
    "\n",
    "df_cleaned['built_year'] = df_cleaned['built_year'].astype(int)\n",
    "df_cleaned['num_beds'] = df_cleaned['num_beds'].astype(int)\n",
    "df_cleaned['num_baths'] = df_cleaned['num_baths'].astype(int)\n",
    "df_cleaned['lng'] = df_cleaned['lng'].astype(np.float16)\n",
    "df_cleaned['lat'] = df_cleaned['lat'].astype(np.float16)\n",
    "\n",
    "df_test_cleaned = df_dirty_test\n",
    "df_test_cleaned['property_type'] = df_test_cleaned['property_type'].str.lower()\n",
    "df_test_cleaned['tenure'] = df_test_cleaned['tenure'].str.lower()\n",
    "df_test_cleaned['furnishing'] = df_test_cleaned['furnishing'].str.lower()\n",
    "df_test_cleaned['subzone'] = df_test_cleaned['subzone'].str.lower()\n",
    "df_test_cleaned['planning_area'] = df_test_cleaned['planning_area'].str.lower()\n",
    "\n",
    "#TODO: Fix comments\n",
    "#df_test_cleaned['built_year'] = df_test_cleaned['built_year'].astype(int)\n",
    "#df_test_cleaned['num_beds'] = df_test_cleaned['num_beds'].astype(int)\n",
    "#df_test_cleaned['num_baths'] = df_test_cleaned['num_baths'].astype(int)\n",
    "df_test_cleaned['lng'] = df_test_cleaned['lng'].astype(np.float16)\n",
    "df_test_cleaned['lat'] = df_test_cleaned['lat'].astype(np.float16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Lease tenure column, Property_Type Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 17220\n",
      "Test size: 6966\n"
     ]
    }
   ],
   "source": [
    "#TENURE COLUMN\n",
    "mask_999 = ['947-year leasehold', '929-year leasehold', '946-year leasehold',\n",
    "'956-year leasehold']\n",
    "mask_99 = ['100-year leasehold', '102-year leasehold', '110-year leasehold', '103-year leasehold']\n",
    "df_cleaned = df_cleaned.replace(mask_999, '999-year leasehold')\n",
    "df_cleaned = df_cleaned.replace(mask_99, '99-year leasehold')\n",
    "\n",
    "df_test_cleaned = df_test_cleaned.replace(mask_999, '999-year leasehold')\n",
    "df_test_cleaned = df_test_cleaned.replace(mask_99, '99-year leasehold')\n",
    "\n",
    "print(f\"Train size: {df_cleaned.shape[0]}\")\n",
    "print(f\"Test size: {df_test_cleaned.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 17219\n",
      "Test size: 6966\n"
     ]
    }
   ],
   "source": [
    "#PROPERTY TYPE\n",
    "# changing \"hdb 3 rooms\", \"hdb 4 rooms\" and likewise to \"hdb\" since the number of rooms info can\n",
    "# be obtained from \"num_beds\"\n",
    "df_cleaned['property_type'].mask(df_cleaned['property_type'].str.contains(\"hdb\"), \"hdb\", inplace=True)\n",
    "df_cleaned.drop(df_cleaned[df_cleaned['property_type']  == 'land only'].index, inplace = True)\n",
    "\n",
    "df_test_cleaned['property_type'].mask(df_test_cleaned['property_type'].str.contains(\"hdb\"), \"hdb\", inplace=True)\n",
    "\n",
    "#TODO: Reduce number of property types with less properties. Maybe do an EDA and figure out best way to remove them. Perhaps club \n",
    "#them in different category.\n",
    "\n",
    "df_cleaned.loc[(df_cleaned['property_type']==\"apartment\") & \n",
    "       (df_cleaned['title'].str.contains('condo')),['property_type']] = \"condo\"\n",
    "\n",
    "df_cleaned.loc[(df_cleaned['property_type']==\"walk-up\") & \n",
    "       (df_cleaned['title'].str.contains('condo')),['property_type']] = \"condo\"\n",
    "\n",
    "df_cleaned.loc[(df_cleaned['property_type']==\"good class bungalow\"),['property_type']] = \"bungalow\"\n",
    "\n",
    "#Test\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"apartment\") & \n",
    "       (df_test_cleaned['title'].str.contains('condo')),['property_type']] = \"condo\"\n",
    "\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"walk-up\") & \n",
    "       (df_test_cleaned['title'].str.contains('condo')),['property_type']] = \"condo\"\n",
    "\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"good class bungalow\"),['property_type']] = \"bungalow\"\n",
    "\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"conservation house\") & \n",
    "       (df_test_cleaned['property_details_url'].str.contains('condo')),['property_type']] = \"condo\"\n",
    "\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"good class bungalow\"),['property_type']] = \"bungalow\"\n",
    "\n",
    "#The following results have been picked manually from the website 99.co\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"conservation house\") & \n",
    "       (df_test_cleaned['property_details_url'].str.contains('blair-plain-conservation-area')),['property_type']] = \"landed\"\n",
    "\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"conservation house\") & \n",
    "       (df_test_cleaned['property_details_url'].str.contains('beng-tong-mansion')),['property_type']] = \"landed\"\n",
    "\n",
    "print(f\"Train size: {df_cleaned.shape[0]}\")\n",
    "print(f\"Test size: {df_test_cleaned.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h. Handle missing values\n",
    "\n",
    "#built_year\n",
    "dfmap = df_cleaned.dropna(subset = ['built_year'])[['built_year', 'property_name']].drop_duplicates()\n",
    "dfmap = dfmap.drop_duplicates(subset = ['property_name'])\n",
    "df_cleaned = df_cleaned.drop(columns=['built_year']).merge(dfmap, on=['property_name'], how='left')\n",
    "\n",
    "dfmap = df_cleaned.dropna(subset = ['tenure'])[['tenure', 'property_name']].drop_duplicates()\n",
    "dfmap = dfmap.drop_duplicates(subset = ['property_name'])\n",
    "df_cleaned = df_cleaned.drop(columns=['tenure']).merge(dfmap, on=['property_name'], how='left')\n",
    "\n",
    "dfmap = df_cleaned.dropna(subset = ['tenure'])[['tenure', 'address']].drop_duplicates()\n",
    "dfmap = dfmap.drop_duplicates(subset = ['address'])\n",
    "df_cleaned = df_cleaned.drop(columns=['tenure']).merge(dfmap, on=['address'], how='left')\n",
    "\n",
    "#df_cleaned.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#h. Handle missing values\n",
    "#print(df_test_cleaned.isnull().sum())\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "#built_year\n",
    "dfmap = df_test_cleaned.dropna(subset = ['built_year'])[['built_year', 'property_name']].drop_duplicates()\n",
    "dfmap = dfmap.drop_duplicates(subset = ['property_name'])\n",
    "df_test_cleaned = df_test_cleaned.drop(columns=['built_year']).merge(dfmap, on=['property_name'], how='left')\n",
    "\n",
    "\n",
    "#df_test_cleaned = df_test_cleaned.dropna(subset=['built_year'])\n",
    "\n",
    "dfmap = df_test_cleaned.dropna(subset = ['tenure'])[['tenure', 'property_name']].drop_duplicates()\n",
    "dfmap = dfmap.drop_duplicates(subset = ['property_name'])\n",
    "df_test_cleaned = df_test_cleaned.drop(columns=['tenure']).merge(dfmap, on=['property_name'], how='left')\n",
    "\n",
    "dfmap = df_test_cleaned.dropna(subset = ['tenure'])[['tenure', 'address']].drop_duplicates()\n",
    "dfmap = dfmap.drop_duplicates(subset = ['address'])\n",
    "df_test_cleaned = df_test_cleaned.drop(columns=['tenure']).merge(dfmap, on=['address'], how='left')\n",
    "\n",
    "#Fill in number of baths from number of bedrooms.\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_baths.isnull() & df_test_cleaned.num_beds == 1), ['num_baths']] = 1\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_baths.isnull() & df_test_cleaned.num_beds == 2), ['num_baths']] = 2\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_baths.isnull() & df_test_cleaned.num_beds > 2), ['num_baths']] = df_test_cleaned.num_beds -1\n",
    "\n",
    "#Fill in number of beds from number of bathrooms.\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_beds.isnull() & df_test_cleaned.num_baths == 1), ['num_beds']] = 1\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_beds.isnull() & df_test_cleaned.num_baths == 2), ['num_beds']] = 2\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_beds.isnull() & df_test_cleaned.num_baths > 2), ['num_beds']] = df_test_cleaned.num_baths + 1\n",
    "\n",
    "#There are 5 properties which have both Nan values for bed and baths. Estimate their beds from area.\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_beds.isnull() & df_test_cleaned.size_sqft < 600), ['num_beds']] = 1\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_beds.isnull() & df_test_cleaned.size_sqft < 1000), ['num_beds']] = 2\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_beds.isnull() & df_test_cleaned.size_sqft < 1700), ['num_beds']] = 3\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_beds.isnull() & df_test_cleaned.size_sqft < 3000), ['num_beds']] = 4\n",
    "\n",
    "#Estimate their baths from beds.\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_baths.isnull() & df_test_cleaned.num_beds == 1), ['num_baths']] = 1\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_baths.isnull() & df_test_cleaned.num_beds == 2), ['num_baths']] = 2\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_baths.isnull() & df_test_cleaned.num_beds > 2), ['num_baths']] = df_test_cleaned.num_beds -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singapore has the following latitude and longitude coordinates in its extreme ends:\n",
    "1. left-most (Tuas) :  1.30871,103.64287\n",
    "2. right-most (Changi) : 1.34538,104.00270\n",
    "3. top-most (Sembawang) : 1.46227,103.79487\n",
    "4. bottom-most (Bukit Merah) : 1.28762,103.82467\n",
    "\n",
    "\n",
    "#### Min latitude - 1.28762       Max latitude - 1.46227\n",
    "\n",
    "#### Min longitude - 103.64         Max longitude - 104.00\n",
    "\n",
    "But we can see that in the data, min longitude is -77.065364 and max latitude is 69.486768 which are out of the range of latitude and longitude values \n",
    "\n",
    "<img src=\"images/singapore-lat-long-map.jpeg\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is interesting to note that in all the records where latitude and longitude have incorrect coordinates, planning_area and subzone have missing values, this can also be verified by checking for count of missing values\n",
      "38 lorong 32 geylang    6\n",
      "17 farrer drive         3\n",
      "15 farrer drive         2\n",
      "Name: address, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_max_lng = df_cleaned[df_cleaned.lng > 121.0]\n",
    "df_min_lng = df_cleaned[df_cleaned.lng < -77.0]\n",
    "df_max_lat = df_cleaned[df_cleaned.lat > 69.0]\n",
    "df_wrong_coordinates = pd.concat([df_max_lng, df_min_lng, df_max_lat])\n",
    "\n",
    "print(\"It is interesting to note that in all the records where latitude and longitude have incorrect coordinates, planning_area and subzone have missing values, this can also be verified by checking for count of missing values\")\n",
    "print(df_wrong_coordinates[\"address\"].value_counts())\n",
    "# coordinates are incorrect for 5 'address'\n",
    "\n",
    "\n",
    "# using the 'address' we can manually correct the latitude, longitude coordinates along with \n",
    "# filling of values for sub zone and planning_area\n",
    "\n",
    "df_cleaned.loc[df_cleaned.address == \"1 tessensohn road\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.3164313, 103.8575321, 'balestier', 'novena'\n",
    "df_cleaned.loc[df_cleaned.address == \"38 lorong 32 geylang\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.31262, 103.88686, 'aljunied', 'geylang'\n",
    "df_cleaned.loc[df_cleaned.address == \"5 jalan mutiara\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.29565, 103.82887, 'leonie hill', 'river valley'\n",
    "df_cleaned.loc[df_cleaned.address == \"17 farrer drive\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.313259, 103.806622, 'holland road', 'bukit timah'\n",
    "df_cleaned.loc[df_cleaned.address == \"15 farrer drive\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.313259, 103.806622, 'holland road', 'bukit timah'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is interesting to note that in all the records where latitude and longitude have incorrect coordinates, planning_area and subzone have missing values, this can also be verified by checking for count of missing values\n",
      "38 lorong 32 geylang    2\n",
      "17 farrer drive         1\n",
      "Name: address, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_max_lng = df_test_cleaned[df_test_cleaned.lng > 121.0]\n",
    "df_min_lng = df_test_cleaned[df_test_cleaned.lng < -77.0]\n",
    "df_max_lat = df_test_cleaned[df_test_cleaned.lat > 69.0]\n",
    "df_wrong_coordinates = pd.concat([df_max_lng, df_min_lng, df_max_lat])\n",
    "\n",
    "print(\"It is interesting to note that in all the records where latitude and longitude have incorrect coordinates, planning_area and subzone have missing values, this can also be verified by checking for count of missing values\")\n",
    "print(df_wrong_coordinates[\"address\"].value_counts())\n",
    "# coordinates are incorrect for 5 'address'\n",
    "\n",
    "\n",
    "# using the 'address' we can manually correct the latitude, longitude coordinates along with \n",
    "# filling of values for sub zone and planning_area\n",
    "\n",
    "df_test_cleaned.loc[df_test_cleaned.address == \"38 lorong 32 geylang\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.31262, 103.88686, 'aljunied', 'geylang'\n",
    "df_test_cleaned.loc[df_test_cleaned.address == \"17 farrer drive\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.313259, 103.806622, 'holland road', 'bukit timah'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging With Auxiliary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrt_station = pd.read_csv('../data/auxiliary-data/sg-mrt-stations.csv')\n",
    "df_mrt_station = df_mrt_station.drop([\"name\", \"lat\", \"lng\", 'planning_area', 'code', 'line', 'opening_year'], axis=1)\n",
    "mrt_station_cleaned = df_mrt_station.value_counts().to_frame(name=\"mrt_station\")\n",
    "\n",
    "mrt_station_coor = pd.read_csv('../data/auxiliary-data/sg-mrt-stations.csv').drop([\"name\", \"subzone\", 'planning_area', 'code', 'line', 'opening_year'], axis=1).to_numpy()\n",
    "mrt_station_coor = np.array([[radians(_) for _ in coor] for coor in mrt_station_coor])\n",
    "\n",
    "df_train_coor = df_cleaned[[\"lat\", \"lng\"]].to_numpy()\n",
    "df_train_coor = np.array([[radians(_) for _ in coor] for coor in df_train_coor])\n",
    "\n",
    "df_test_coor = df_test_cleaned[[\"lat\", \"lng\"]].to_numpy()\n",
    "df_test_coor = np.array([[radians(_) for _ in coor] for coor in df_test_coor])\n",
    "\n",
    "dist_matrix_train = sklearn.metrics.pairwise.haversine_distances(mrt_station_coor, df_train_coor)\n",
    "# multiply to get meters\n",
    "closest_dist_to_mrt_train = pd.DataFrame(np.amin(dist_matrix_train, axis=0)* 6371000, columns=[\"closest_dist_to_mrt\"])\n",
    "\n",
    "dist_matrix_test = sklearn.metrics.pairwise.haversine_distances(mrt_station_coor, df_test_coor)\n",
    "# multiply to get meters\n",
    "closest_dist_to_mrt_test = pd.DataFrame(np.amin(dist_matrix_test, axis=0)* 6371000, columns=[\"closest_dist_to_mrt\"])\n",
    "\n",
    "df_cleaned = df_cleaned.merge(pri_sch_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(sec_sch_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(mrt_station_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(df_subzone, how='left',left_on=\"subzone\",right_on=\"name\")\\\n",
    "    .join(closest_dist_to_mrt_train)\\\n",
    "    .fillna({'pri_sch':0, 'sec_sch':0, 'mrt_station':0})\n",
    "\n",
    "df_test_cleaned = df_test_cleaned.merge(pri_sch_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(sec_sch_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(mrt_station_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(df_subzone, how='left',left_on=\"subzone\",right_on=\"name\")\\\n",
    "    .join(closest_dist_to_mrt_test)\\\n",
    "    .fillna({'pri_sch':0, 'sec_sch':0, 'mrt_station':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Columns at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.drop(['title', 'address','property_name', 'property_details_url', 'listing_id', 'elevation', 'total_num_units', 'floor_level', 'available_unit_types'], axis = 1)\n",
    "df_test_cleaned = df_test_cleaned.drop(['title','address','property_name', 'property_details_url', 'listing_id', 'elevation', 'total_num_units', 'floor_level', 'available_unit_types'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property_type          0\n",
      "num_beds               0\n",
      "num_baths              0\n",
      "size_sqft              0\n",
      "furnishing             0\n",
      "lat                    0\n",
      "lng                    0\n",
      "subzone                0\n",
      "planning_area          0\n",
      "price                  0\n",
      "built_year             0\n",
      "tenure                 0\n",
      "pri_sch                0\n",
      "sec_sch                0\n",
      "mrt_station            0\n",
      "population_density     0\n",
      "closest_dist_to_mrt    0\n",
      "dtype: int64\n",
      "property_type            0\n",
      "num_beds                 0\n",
      "num_baths                0\n",
      "size_sqft                0\n",
      "furnishing               0\n",
      "lat                      0\n",
      "lng                      0\n",
      "subzone                 30\n",
      "planning_area           30\n",
      "built_year             341\n",
      "tenure                  78\n",
      "pri_sch                  0\n",
      "sec_sch                  0\n",
      "mrt_station              0\n",
      "population_density      30\n",
      "closest_dist_to_mrt      0\n",
      "dtype: int64\n",
      "(17219, 17)\n",
      "(6966, 16)\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned.isnull().sum())\n",
    "print(df_test_cleaned.isnull().sum())\n",
    "print(df_cleaned.shape)\n",
    "print(df_test_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cleaned.to_csv('../data/test_cleaned.csv', index = False)\n",
    "df_cleaned.to_csv('../data/train_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "property_type          0\n",
       "num_beds               0\n",
       "num_baths              0\n",
       "size_sqft              0\n",
       "furnishing             0\n",
       "lat                    0\n",
       "lng                    0\n",
       "subzone                0\n",
       "planning_area          0\n",
       "price                  0\n",
       "built_year             0\n",
       "tenure                 0\n",
       "pri_sch                0\n",
       "sec_sch                0\n",
       "mrt_station            0\n",
       "population_density     0\n",
       "closest_dist_to_mrt    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6966, 16)"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_cleaned.isnull().sum()\n",
    "df_test_cleaned.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
