{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import radians\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "print(f\"Train size: {df_train.shape[0]}\")\n",
    "print(f\"Test size: {df_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Primary School"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.median(df_test[df_test[\"subzone\"] == \"serangoon garden\"][\"built_year\"].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we fill in empty build year of properties with median of built years of the properties in the same subzone\n",
    "for row in df_test.index:\n",
    "    if np.isnan(df_test.loc[row, \"built_year\"]):\n",
    "        df_test.loc[row, \"built_year\"] = np.median(df_test[df_test[\"subzone\"] == df_test.loc[row, \"subzone\"]][\"built_year\"].dropna())\n",
    "\n",
    "print(df_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"We can see that there are still 20 properties without built year.\")\n",
    "df_test[np.isnan(df_test['built_year'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#quick search of data online\n",
    "df_test.loc[df_test[\"title\"].str.contains(\"copen grand\"), 'built_year']=2027\n",
    "df_test.loc[df_test[\"title\"].str.contains(\"jurong park\"), 'built_year']=1971\n",
    "df_test.loc[df_test[\"title\"].str.contains(\"ponggol park\"), 'built_year']=2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_test[pd.isnull(df_test['subzone'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_test.loc[df_test[\"address\"].str.contains(\"1 tessensohn road\", na=False), 'subzone'] = 'lavender'\n",
    "df_test.loc[df_test[\"address\"].str.contains(\"1 tessensohn road\", na=False), 'planning_area'] = 'kallang'\n",
    "\n",
    "df_test.loc[df_test[\"address\"].str.contains(\"38 lorong 32 geylang\", na=False), 'subzone'] = 'aljunied'\n",
    "df_test.loc[df_test[\"address\"].str.contains(\"38 lorong 32 geylang\", na=False), 'planning_area'] = 'geylang'\n",
    "\n",
    "df_test.loc[df_test[\"address\"].str.contains(\"17 farrer drive\", na=False), 'subzone'] = 'leonie hill'\n",
    "df_test.loc[df_test[\"address\"].str.contains(\"17 farrer drive\", na=False), 'planning_area'] = 'river valley'\n",
    "\n",
    "df_test.loc[df_test[\"address\"].str.contains(\"5 jalan mutiara\", na=False), 'subzone'] = 'central subzone'\n",
    "df_test.loc[df_test[\"address\"].str.contains(\"5 jalan mutiara\", na=False), 'planning_area'] = 'downtown core'\n",
    "\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_pri_sch = pd.read_csv('../data/auxiliary-data/sg-primary-schools.csv')\n",
    "df_pri_sch = df_pri_sch.drop([\"name\", \"lat\", \"lng\", 'planning_area'], axis=1)\n",
    "pri_sch_cleaned = df_pri_sch.value_counts().to_frame(name=\"pri_sch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Secondary School"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_sec_sch = pd.read_csv('../data/auxiliary-data/sg-secondary-schools.csv')\n",
    "df_sec_sch = df_sec_sch.drop([\"name\", \"lat\", \"lng\", 'planning_area'], axis=1)\n",
    "sec_sch_cleaned = df_sec_sch.value_counts().to_frame(name=\"sec_sch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Population Density of Subzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_subzone = pd.read_csv('../data/auxiliary-data/sg-subzones.csv')\n",
    "\n",
    "for row in df_subzone.index:\n",
    "    if df_subzone.loc[row, \"population\"] == 0:\n",
    "        df_subzone.loc[row, \"population\"] += 10\n",
    "\n",
    "df_subzone[\"population_density\"] = df_subzone['population']/df_subzone[\"area_size\"]\n",
    "df_subzone = df_subzone.drop(['area_size', 'population', 'planning_area'],axis=1).set_index(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_dirty_test = df_test\n",
    "df_dirty_train = df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remove duplicates and invalid data (Rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned = df_dirty_train.drop_duplicates()\n",
    "df_cleaned = df_cleaned[df_cleaned.size_sqft > 0]\n",
    "df_cleaned.dropna(subset=['num_beds', 'num_baths', 'price', 'size_sqft', 'built_year', 'available_unit_types', 'tenure'], inplace = True)\n",
    "df_cleaned = df_cleaned[df_cleaned.price > 0]\n",
    "df_cleaned = df_cleaned[df_cleaned.furnishing != \"na\"]\n",
    "print(f'Records dropped :{df_dirty_train.shape[0] - df_cleaned.shape[0]}' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#c.\tStandardize capitalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned['property_type'] = df_cleaned['property_type'].str.lower()\n",
    "df_cleaned['tenure'] = df_cleaned['tenure'].str.lower()\n",
    "df_cleaned['furnishing'] = df_cleaned['furnishing'].str.lower()\n",
    "df_cleaned['subzone'] = df_cleaned['subzone'].str.lower()\n",
    "df_cleaned['planning_area'] = df_cleaned['planning_area'].str.lower()\n",
    "\n",
    "df_cleaned['built_year'] = df_cleaned['built_year'].astype(int)\n",
    "df_cleaned['num_beds'] = df_cleaned['num_beds'].astype(int)\n",
    "df_cleaned['num_baths'] = df_cleaned['num_baths'].astype(int)\n",
    "df_cleaned['lng'] = df_cleaned['lng'].astype(np.float16)\n",
    "df_cleaned['lat'] = df_cleaned['lat'].astype(np.float16)\n",
    "\n",
    "df_test_cleaned = df_dirty_test\n",
    "df_test_cleaned['property_type'] = df_test_cleaned['property_type'].str.lower()\n",
    "df_test_cleaned['tenure'] = df_test_cleaned['tenure'].str.lower()\n",
    "df_test_cleaned['furnishing'] = df_test_cleaned['furnishing'].str.lower()\n",
    "df_test_cleaned['subzone'] = df_test_cleaned['subzone'].str.lower()\n",
    "df_test_cleaned['planning_area'] = df_test_cleaned['planning_area'].str.lower()\n",
    "\n",
    "#TODO: Fix comments\n",
    "#df_test_cleaned['built_year'] = df_test_cleaned['built_year'].astype(int)\n",
    "#df_test_cleaned['num_beds'] = df_test_cleaned['num_beds'].astype(int)\n",
    "#df_test_cleaned['num_baths'] = df_test_cleaned['num_baths'].astype(int)\n",
    "df_test_cleaned['lng'] = df_test_cleaned['lng'].astype(np.float16)\n",
    "df_test_cleaned['lat'] = df_test_cleaned['lat'].astype(np.float16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Clean Lease tenure column, Property_Type Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#TENURE COLUMN\n",
    "mask_999 = ['947-year leasehold', '929-year leasehold', '946-year leasehold',\n",
    "'956-year leasehold']\n",
    "mask_99 = ['100-year leasehold', '102-year leasehold', '110-year leasehold', '103-year leasehold']\n",
    "df_cleaned = df_cleaned.replace(mask_999, '999-year leasehold')\n",
    "df_cleaned = df_cleaned.replace(mask_99, '99-year leasehold')\n",
    "\n",
    "df_test_cleaned = df_test_cleaned.replace(mask_999, '999-year leasehold')\n",
    "df_test_cleaned = df_test_cleaned.replace(mask_99, '99-year leasehold')\n",
    "\n",
    "print(f\"Train size: {df_cleaned.shape[0]}\")\n",
    "print(f\"Test size: {df_test_cleaned.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#PROPERTY TYPE\n",
    "# changing \"hdb 3 rooms\", \"hdb 4 rooms\" and likewise to \"hdb\" since the number of rooms info can\n",
    "# be obtained from \"num_beds\"\n",
    "df_cleaned['property_type'].mask(df_cleaned['property_type'].str.contains(\"hdb\"), \"hdb\", inplace=True)\n",
    "df_cleaned.drop(df_cleaned[df_cleaned['property_type']  == 'land only'].index, inplace = True)\n",
    "\n",
    "df_test_cleaned['property_type'].mask(df_test_cleaned['property_type'].str.contains(\"hdb\"), \"hdb\", inplace=True)\n",
    "\n",
    "#TODO: Reduce number of property types with less properties. Maybe do an EDA and figure out best way to remove them. Perhaps club \n",
    "#them in different category.\n",
    "\n",
    "df_cleaned.loc[(df_cleaned['property_type']==\"apartment\") & \n",
    "       (df_cleaned['title'].str.contains('condo')),['property_type']] = \"condo\"\n",
    "\n",
    "df_cleaned.loc[(df_cleaned['property_type']==\"walk-up\") & \n",
    "       (df_cleaned['title'].str.contains('condo')),['property_type']] = \"condo\"\n",
    "\n",
    "df_cleaned.loc[(df_cleaned['property_type']==\"good class bungalow\"),['property_type']] = \"bungalow\"\n",
    "\n",
    "#Test\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"apartment\") & \n",
    "       (df_test_cleaned['title'].str.contains('condo')),['property_type']] = \"condo\"\n",
    "\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"walk-up\") & \n",
    "       (df_test_cleaned['title'].str.contains('condo')),['property_type']] = \"condo\"\n",
    "\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"good class bungalow\"),['property_type']] = \"bungalow\"\n",
    "\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"conservation house\") & \n",
    "       (df_test_cleaned['property_details_url'].str.contains('condo')),['property_type']] = \"condo\"\n",
    "\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"good class bungalow\"),['property_type']] = \"bungalow\"\n",
    "\n",
    "#The following results have been picked manually from the website 99.co\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"conservation house\") & \n",
    "       (df_test_cleaned['property_details_url'].str.contains('blair-plain-conservation-area')),['property_type']] = \"landed\"\n",
    "\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"conservation house\") & \n",
    "       (df_test_cleaned['property_details_url'].str.contains('beng-tong-mansion')),['property_type']] = \"landed\"\n",
    "\n",
    "print(f\"Train size: {df_cleaned.shape[0]}\")\n",
    "print(f\"Test size: {df_test_cleaned.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#h. Handle missing values\n",
    "\n",
    "#built_year\n",
    "dfmap = df_cleaned.dropna(subset = ['built_year'])[['built_year', 'property_name']].drop_duplicates()\n",
    "dfmap = dfmap.drop_duplicates(subset = ['property_name'])\n",
    "df_cleaned = df_cleaned.drop(columns=['built_year']).merge(dfmap, on=['property_name'], how='left')\n",
    "\n",
    "dfmap = df_cleaned.dropna(subset = ['tenure'])[['tenure', 'property_name']].drop_duplicates()\n",
    "dfmap = dfmap.drop_duplicates(subset = ['property_name'])\n",
    "df_cleaned = df_cleaned.drop(columns=['tenure']).merge(dfmap, on=['property_name'], how='left')\n",
    "\n",
    "dfmap = df_cleaned.dropna(subset = ['tenure'])[['tenure', 'address']].drop_duplicates()\n",
    "dfmap = dfmap.drop_duplicates(subset = ['address'])\n",
    "df_cleaned = df_cleaned.drop(columns=['tenure']).merge(dfmap, on=['address'], how='left')\n",
    "\n",
    "#df_cleaned.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#h. Handle missing values\n",
    "#print(df_test_cleaned.isnull().sum())\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "#built_year\n",
    "dfmap = df_test_cleaned.dropna(subset = ['built_year'])[['built_year', 'property_name']].drop_duplicates()\n",
    "dfmap = dfmap.drop_duplicates(subset = ['property_name'])\n",
    "df_test_cleaned = df_test_cleaned.drop(columns=['built_year']).merge(dfmap, on=['property_name'], how='left')\n",
    "\n",
    "\n",
    "#df_test_cleaned = df_test_cleaned.dropna(subset=['built_year'])\n",
    "\n",
    "dfmap = df_test_cleaned.dropna(subset = ['tenure'])[['tenure', 'property_name']].drop_duplicates()\n",
    "dfmap = dfmap.drop_duplicates(subset = ['property_name'])\n",
    "df_test_cleaned = df_test_cleaned.drop(columns=['tenure']).merge(dfmap, on=['property_name'], how='left')\n",
    "\n",
    "dfmap = df_test_cleaned.dropna(subset = ['tenure'])[['tenure', 'address']].drop_duplicates()\n",
    "dfmap = dfmap.drop_duplicates(subset = ['address'])\n",
    "df_test_cleaned = df_test_cleaned.drop(columns=['tenure']).merge(dfmap, on=['address'], how='left')\n",
    "\n",
    "#Fill in number of baths from number of bedrooms.\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_baths.isnull() & df_test_cleaned.num_beds == 1), ['num_baths']] = 1\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_baths.isnull() & df_test_cleaned.num_beds == 2), ['num_baths']] = 2\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_baths.isnull() & df_test_cleaned.num_beds > 2), ['num_baths']] = df_test_cleaned.num_beds -1\n",
    "\n",
    "#Fill in number of beds from number of bathrooms.\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_beds.isnull() & df_test_cleaned.num_baths == 1), ['num_beds']] = 1\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_beds.isnull() & df_test_cleaned.num_baths == 2), ['num_beds']] = 2\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_beds.isnull() & df_test_cleaned.num_baths > 2), ['num_beds']] = df_test_cleaned.num_baths + 1\n",
    "\n",
    "#There are 5 properties which have both Nan values for bed and baths. Estimate their beds from area.\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_beds.isnull() & df_test_cleaned.size_sqft < 600), ['num_beds']] = 1\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_beds.isnull() & df_test_cleaned.size_sqft < 1000), ['num_beds']] = 2\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_beds.isnull() & df_test_cleaned.size_sqft < 1700), ['num_beds']] = 3\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_beds.isnull() & df_test_cleaned.size_sqft < 3000), ['num_beds']] = 4\n",
    "\n",
    "#Estimate their baths from beds.\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_baths.isnull() & df_test_cleaned.num_beds == 1), ['num_baths']] = 1\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_baths.isnull() & df_test_cleaned.num_beds == 2), ['num_baths']] = 2\n",
    "df_test_cleaned.loc[(df_test_cleaned.num_baths.isnull() & df_test_cleaned.num_beds > 2), ['num_baths']] = df_test_cleaned.num_beds -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#fill in missing tenure type for HDB\n",
    "\n",
    "df_test_cleaned.loc[(df_test_cleaned[\"property_type\"] == \"hdb\") & (df_test_cleaned[\"tenure\"].isnull()), \"tenure\"] = \"99-year leasehold\"\n",
    "df_test_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# find the most probable type of tenure from the train data, use them to fill in the unknown tenure type\n",
    "df_cleaned[df_cleaned[\"property_type\"] == \"condo\"][\"tenure\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned[df_cleaned[\"property_type\"] == \"terraced house\"][\"tenure\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned[df_cleaned[\"property_type\"] == \"semi-detached house\"][\"tenure\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned[df_cleaned[\"property_type\"] == \"bungalow\"][\"tenure\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned[df_cleaned[\"property_type\"] == \"corner terrace\"][\"tenure\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned[df_cleaned[\"property_type\"] == \"landed\"][\"tenure\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned[df_cleaned[\"property_type\"] == \"cluster house\"][\"tenure\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_test_cleaned.loc[(df_test_cleaned[\"property_type\"] == \"condo\") & (df_test_cleaned[\"tenure\"].isnull()), \"tenure\"] = \"99-year leasehold\"\n",
    "df_test_cleaned.loc[(df_test_cleaned[\"property_type\"] == \"landed\") & (df_test_cleaned[\"tenure\"].isnull()), \"tenure\"] = \"99-year leasehold\"\n",
    "df_test_cleaned.loc[(df_test_cleaned[\"property_type\"] == \"cluster house\") & (df_test_cleaned[\"tenure\"].isnull()), \"tenure\"] = \"99-year leasehold\"\n",
    "df_test_cleaned.loc[(df_test_cleaned[\"property_type\"] == \"terraced house\") & (df_test_cleaned[\"tenure\"].isnull()), \"tenure\"] = \"freehold\"\n",
    "df_test_cleaned.loc[(df_test_cleaned[\"property_type\"] == \"semi-detached house\") & (df_test_cleaned[\"tenure\"].isnull()), \"tenure\"] = \"freehold\"\n",
    "df_test_cleaned.loc[(df_test_cleaned[\"property_type\"] == \"bungalow\") & (df_test_cleaned[\"tenure\"].isnull()), \"tenure\"] = \"freehold\"\n",
    "df_test_cleaned.loc[(df_test_cleaned[\"property_type\"] == \"corner terrace\") & (df_test_cleaned[\"tenure\"].isnull()), \"tenure\"] = \"freehold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# there are only 2 records which appear to be incorrect as num_beds =1, whereas num_baths = 10 and price is > 3e8\n",
    "# hence we can drop these two records so that the results are not skewed\n",
    "\n",
    "df_cleaned = df_cleaned[df_cleaned['price']<3e8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Singapore has the following latitude and longitude coordinates in its extreme ends:\n",
    "1. left-most (Tuas) :  1.30871,103.64287\n",
    "2. right-most (Changi) : 1.34538,104.00270\n",
    "3. top-most (Sembawang) : 1.46227,103.79487\n",
    "4. bottom-most (Bukit Merah) : 1.28762,103.82467\n",
    "\n",
    "\n",
    "#### Min latitude - 1.28762       Max latitude - 1.46227\n",
    "\n",
    "#### Min longitude - 103.64         Max longitude - 104.00\n",
    "\n",
    "But we can see that in the data, min longitude is -77.065364 and max latitude is 69.486768 which are out of the range of latitude and longitude values \n",
    "\n",
    "<img src=\"images/singapore-lat-long-map.jpeg\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_max_lng = df_cleaned[df_cleaned.lng > 121.0]\n",
    "df_min_lng = df_cleaned[df_cleaned.lng < -77.0]\n",
    "df_max_lat = df_cleaned[df_cleaned.lat > 69.0]\n",
    "df_wrong_coordinates = pd.concat([df_max_lng, df_min_lng, df_max_lat])\n",
    "\n",
    "print(\"It is interesting to note that in all the records where latitude and longitude have incorrect coordinates, planning_area and subzone have missing values, this can also be verified by checking for count of missing values\")\n",
    "print(df_wrong_coordinates[\"address\"].value_counts())\n",
    "# coordinates are incorrect for 5 'address'\n",
    "\n",
    "\n",
    "# using the 'address' we can manually correct the latitude, longitude coordinates along with \n",
    "# filling of values for sub zone and planning_area\n",
    "\n",
    "df_cleaned.loc[df_cleaned.address == \"1 tessensohn road\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.3164313, 103.8575321, 'balestier', 'novena'\n",
    "df_cleaned.loc[df_cleaned.address == \"38 lorong 32 geylang\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.31262, 103.88686, 'aljunied', 'geylang'\n",
    "df_cleaned.loc[df_cleaned.address == \"5 jalan mutiara\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.29565, 103.82887, 'leonie hill', 'river valley'\n",
    "df_cleaned.loc[df_cleaned.address == \"17 farrer drive\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.313259, 103.806622, 'holland road', 'bukit timah'\n",
    "df_cleaned.loc[df_cleaned.address == \"15 farrer drive\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.313259, 103.806622, 'holland road', 'bukit timah'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_max_lng = df_test_cleaned[df_test_cleaned.lng > 121.0]\n",
    "df_min_lng = df_test_cleaned[df_test_cleaned.lng < -77.0]\n",
    "df_max_lat = df_test_cleaned[df_test_cleaned.lat > 69.0]\n",
    "df_wrong_coordinates = pd.concat([df_max_lng, df_min_lng, df_max_lat])\n",
    "\n",
    "print(\"It is interesting to note that in all the records where latitude and longitude have incorrect coordinates, planning_area and subzone have missing values, this can also be verified by checking for count of missing values\")\n",
    "print(df_wrong_coordinates[\"address\"].value_counts())\n",
    "# coordinates are incorrect for 5 'address'\n",
    "\n",
    "\n",
    "# using the 'address' we can manually correct the latitude, longitude coordinates along with \n",
    "# filling of values for sub zone and planning_area\n",
    "\n",
    "df_test_cleaned.loc[df_test_cleaned.address == \"38 lorong 32 geylang\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.31262, 103.88686, 'aljunied', 'geylang'\n",
    "df_test_cleaned.loc[df_test_cleaned.address == \"17 farrer drive\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.313259, 103.806622, 'holland road', 'bukit timah'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Merging With Auxiliary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_pri_sch = pd.read_csv('../data/auxiliary-data/sg-primary-schools.csv').drop([\"name\", \"subzone\", 'planning_area'], axis=1).to_numpy()\n",
    "pri_sch_coor = np.array([[radians(_) for _ in coor] for coor in df_pri_sch])\n",
    "pri_sch_coor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_sec_sch = pd.read_csv('../data/auxiliary-data/sg-secondary-schools.csv').drop([\"name\", \"subzone\", 'planning_area'], axis=1).to_numpy()\n",
    "sec_sch_coor = np.array([[radians(_) for _ in coor] for coor in df_sec_sch])\n",
    "sec_sch_coor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_commercial_centres = pd.read_csv('../data/auxiliary-data/sg-commerical-centres.csv').drop([\"name\", \"type\", \"subzone\", 'planning_area'], axis=1).to_numpy()\n",
    "commercial_centres_coor = np.array([[radians(_) for _ in coor] for coor in df_commercial_centres])\n",
    "\n",
    "df_shopping_malls = pd.read_csv('../data/auxiliary-data/sg-shopping-malls.csv').drop([\"name\", \"subzone\", 'planning_area'], axis=1).to_numpy()\n",
    "shopping_malls_coor = np.array([[radians(_) for _ in coor] for coor in df_shopping_malls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_mrt_station = pd.read_csv('../data/auxiliary-data/sg-mrt-stations.csv')\n",
    "df_mrt_station = df_mrt_station.drop([\"name\", \"lat\", \"lng\", 'planning_area', 'code', 'line', 'opening_year'], axis=1)\n",
    "mrt_station_cleaned = df_mrt_station.value_counts().to_frame(name=\"mrt_station\")\n",
    "\n",
    "mrt_station_coor = pd.read_csv('../data/auxiliary-data/sg-mrt-stations.csv').drop([\"name\", \"subzone\", 'planning_area', 'code', 'line', 'opening_year'], axis=1).to_numpy()\n",
    "mrt_station_coor = np.array([[radians(_) for _ in coor] for coor in mrt_station_coor])\n",
    "\n",
    "df_train_coor = df_cleaned[[\"lat\", \"lng\"]].to_numpy()\n",
    "df_train_coor = np.array([[radians(_) for _ in coor] for coor in df_train_coor])\n",
    "\n",
    "df_test_coor = df_test_cleaned[[\"lat\", \"lng\"]].to_numpy()\n",
    "df_test_coor = np.array([[radians(_) for _ in coor] for coor in df_test_coor])\n",
    "\n",
    "dist_matrix_train_mrt = sklearn.metrics.pairwise.haversine_distances(mrt_station_coor, df_train_coor)\n",
    "# multiply to get meters\n",
    "closest_dist_to_mrt_train = pd.DataFrame(np.amin(dist_matrix_train_mrt, axis=0)* 6371000, columns=[\"closest_dist_to_mrt\"])\n",
    "\n",
    "dist_matrix_test_mrt = sklearn.metrics.pairwise.haversine_distances(mrt_station_coor, df_test_coor)\n",
    "# multiply to get meters\n",
    "closest_dist_to_mrt_test = pd.DataFrame(np.amin(dist_matrix_test_mrt, axis=0)* 6371000, columns=[\"closest_dist_to_mrt\"])\n",
    "\n",
    "dist_matrix_train_pri = sklearn.metrics.pairwise.haversine_distances(pri_sch_coor, df_train_coor)\n",
    "closest_dist_to_pri_train = pd.DataFrame(np.amin(dist_matrix_train_pri, axis=0)* 6371000, columns=[\"closest_dist_to_pri\"])\n",
    "\n",
    "dist_matrix_test_pri = sklearn.metrics.pairwise.haversine_distances(pri_sch_coor, df_test_coor)\n",
    "closest_dist_to_pri_test = pd.DataFrame(np.amin(dist_matrix_test_pri, axis=0)* 6371000, columns=[\"closest_dist_to_pri\"])\n",
    "\n",
    "dist_matrix_train_sec = sklearn.metrics.pairwise.haversine_distances(sec_sch_coor, df_train_coor)\n",
    "closest_dist_to_sec_train = pd.DataFrame(np.amin(dist_matrix_train_sec, axis=0)* 6371000, columns=[\"closest_dist_to_sec\"])\n",
    "\n",
    "dist_matrix_test_sec = sklearn.metrics.pairwise.haversine_distances(sec_sch_coor, df_test_coor)\n",
    "closest_dist_to_sec_test = pd.DataFrame(np.amin(dist_matrix_test_sec, axis=0)* 6371000, columns=[\"closest_dist_to_sec\"])\n",
    "\n",
    "dist_matrix_test_com = sklearn.metrics.pairwise.haversine_distances(commercial_centres_coor, df_test_coor)\n",
    "closest_dist_to_com_test = pd.DataFrame(np.amin(dist_matrix_test_com, axis=0)* 6371000, columns=[\"closest_dist_to_com\"])\n",
    "\n",
    "dist_matrix_test_shop = sklearn.metrics.pairwise.haversine_distances(shopping_malls_coor, df_test_coor)\n",
    "closest_dist_to_shop_test = pd.DataFrame(np.amin(dist_matrix_test_shop, axis=0)* 6371000, columns=[\"closest_dist_to_shop\"])\n",
    "\n",
    "dist_matrix_train_com = sklearn.metrics.pairwise.haversine_distances(commercial_centres_coor, df_train_coor)\n",
    "closest_dist_to_com_train = pd.DataFrame(np.amin(dist_matrix_train_com, axis=0)* 6371000, columns=[\"closest_dist_to_com\"])\n",
    "\n",
    "dist_matrix_train_shop = sklearn.metrics.pairwise.haversine_distances(shopping_malls_coor, df_train_coor)\n",
    "closest_dist_to_shop_train = pd.DataFrame(np.amin(dist_matrix_train_shop, axis=0)* 6371000, columns=[\"closest_dist_to_shop\"])\n",
    "\n",
    "#during pri sch registration exercise, homeowners within 1km will be given priority\n",
    "test = pd.DataFrame(dist_matrix_test_pri * 6371000)\n",
    "test_near_pri_sch = test[test <= 1000].count().rename(\"close_pri_sch\")\n",
    "\n",
    "test = pd.DataFrame(dist_matrix_train_pri * 6371000)\n",
    "train_near_pri_sch = test[test <= 1000].count().rename(\"close_pri_sch\")\n",
    "\n",
    "#no such priority for sec sch, put as 1km for now\n",
    "#change 1000 to desired distance if necessary\n",
    "test = pd.DataFrame(dist_matrix_test_sec * 6371000)\n",
    "test_near_sec_sch = test[test <= 1000].count().rename(\"close_sec_sch\")\n",
    "\n",
    "test = pd.DataFrame(dist_matrix_train_sec * 6371000)\n",
    "train_near_sec_sch = test[test <= 1000].count().rename(\"close_sec_sch\")\n",
    "\n",
    "df_cleaned = df_cleaned.merge(pri_sch_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(sec_sch_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(mrt_station_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(df_subzone, how='left',left_on=\"subzone\",right_on=\"name\")\\\n",
    "    .join(closest_dist_to_mrt_train)\\\n",
    "    .join(closest_dist_to_pri_train)\\\n",
    "    .join(closest_dist_to_sec_train)\\\n",
    "    .join(train_near_pri_sch)\\\n",
    "    .join(train_near_sec_sch)\\\n",
    "    .join(closest_dist_to_shop_train)\\\n",
    "    .join(closest_dist_to_com_train)\\\n",
    "    .fillna({'pri_sch':0, 'sec_sch':0, 'mrt_station':0})\n",
    "\n",
    "df_test_cleaned = df_test_cleaned.merge(pri_sch_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(sec_sch_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(mrt_station_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(df_subzone, how='left',left_on=\"subzone\",right_on=\"name\")\\\n",
    "    .join(closest_dist_to_mrt_test)\\\n",
    "    .join(closest_dist_to_pri_test)\\\n",
    "    .join(closest_dist_to_sec_test)\\\n",
    "    .join(test_near_pri_sch)\\\n",
    "    .join(test_near_sec_sch)\\\n",
    "    .join(closest_dist_to_shop_test)\\\n",
    "    .join(closest_dist_to_com_test)\\\n",
    "    .fillna({'pri_sch':0, 'sec_sch':0, 'mrt_station':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Drop Columns at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.drop(['title', 'address','property_name', 'property_details_url', 'listing_id', 'elevation', 'total_num_units', 'floor_level', 'available_unit_types'], axis = 1)\n",
    "df_test_cleaned = df_test_cleaned.drop(['title','address','property_name', 'property_details_url', 'listing_id', 'elevation', 'total_num_units', 'floor_level', 'available_unit_types'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_cleaned.isnull().sum())\n",
    "print(df_test_cleaned.isnull().sum())\n",
    "print(df_cleaned.shape)\n",
    "print(df_test_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_test_cleaned.to_csv('../data/test_cleaned.csv', index = False)\n",
    "df_cleaned.to_csv('../data/train_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_test_cleaned.isnull().sum()\n",
    "df_test_cleaned.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
