{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import radians\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 20254\n",
      "Test size: 6966\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "print(f\"Train size: {df_train.shape[0]}\")\n",
    "print(f\"Test size: {df_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Primary School"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subzone    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# count the number of primary schools per subzone\n",
    "df_pri_sch = pd.read_csv('../data/auxiliary-data/sg-primary-schools.csv')\n",
    "df_pri_sch = df_pri_sch.drop([\"name\", \"lat\", \"lng\", 'planning_area'], axis=1)\n",
    "pri_sch_cleaned = df_pri_sch.value_counts().to_frame(name=\"pri_sch\")\n",
    "print(df_pri_sch.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Secondary School"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# count the number of secondary schools per subzone\n",
    "df_sec_sch = pd.read_csv('../data/auxiliary-data/sg-secondary-schools.csv')\n",
    "df_sec_sch = df_sec_sch.drop([\"name\", \"lat\", \"lng\", 'planning_area'], axis=1)\n",
    "sec_sch_cleaned = df_sec_sch.value_counts().to_frame(name=\"sec_sch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Population Density of Subzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population_density    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# get population density per subzone\n",
    "df_subzone = pd.read_csv('../data/auxiliary-data/sg-subzones.csv')\n",
    "\n",
    "## replace 0 population with 10 to give some non-zero population density\n",
    "# df_subzone.loc[df_subzone['population'] == 0, 'population'] = 10\n",
    "\n",
    "df_subzone[\"population_density\"] = df_subzone['population']/df_subzone[\"area_size\"].replace(0, np.nan)\n",
    "df_subzone = df_subzone.drop(['area_size', 'population', 'planning_area'],axis=1).set_index(\"name\")\n",
    "print(df_subzone.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_dirty_test = df_test.copy()\n",
    "df_dirty_train = df_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remove duplicates and invalid data (Rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records dropped :1824\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = df_dirty_train.drop_duplicates()\n",
    "df_cleaned = df_cleaned[df_cleaned.size_sqft > 0].copy()\n",
    "df_cleaned.dropna(subset=['price', 'size_sqft', 'tenure'], inplace = True)\n",
    "df_cleaned = df_cleaned[(df_cleaned.price > 0)].copy()\n",
    "print(f'Records dropped :{df_dirty_train.shape[0] - df_cleaned.shape[0]}')\n",
    "#num_beds, num_baths, furnishing, built_year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#c.\tStandardize capitalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned['property_type'] = df_cleaned['property_type'].str.lower()\n",
    "df_cleaned['tenure'] = df_cleaned['tenure'].str.lower()\n",
    "df_cleaned['furnishing'] = df_cleaned['furnishing'].str.lower()\n",
    "df_cleaned['subzone'] = df_cleaned['subzone'].str.lower()\n",
    "df_cleaned['planning_area'] = df_cleaned['planning_area'].str.lower()\n",
    "\n",
    "#df_cleaned['built_year'] = df_cleaned['built_year'].astype(int)\n",
    "#df_cleaned['num_beds'] = df_cleaned['num_beds'].astype(int)\n",
    "#df_cleaned['num_baths'] = df_cleaned['num_baths'].astype(int)\n",
    "df_cleaned['lng'] = df_cleaned['lng'].astype(np.float64)\n",
    "df_cleaned['lat'] = df_cleaned['lat'].astype(np.float64)\n",
    "\n",
    "df_test_cleaned = df_dirty_test.copy()\n",
    "df_test_cleaned['property_type'] = df_test_cleaned['property_type'].str.lower()\n",
    "df_test_cleaned['tenure'] = df_test_cleaned['tenure'].str.lower()\n",
    "df_test_cleaned['furnishing'] = df_test_cleaned['furnishing'].str.lower()\n",
    "df_test_cleaned['subzone'] = df_test_cleaned['subzone'].str.lower()\n",
    "df_test_cleaned['planning_area'] = df_test_cleaned['planning_area'].str.lower()\n",
    "\n",
    "#TODO: Fix comments\n",
    "#df_test_cleaned['built_year'] = df_test_cleaned['built_year'].astype(int)\n",
    "#df_test_cleaned['num_beds'] = df_test_cleaned['num_beds'].astype(int)\n",
    "#df_test_cleaned['num_baths'] = df_test_cleaned['num_baths'].astype(int)\n",
    "df_test_cleaned['lng'] = df_test_cleaned['lng'].astype(np.float64)\n",
    "df_test_cleaned['lat'] = df_test_cleaned['lat'].astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Clean Lease tenure column, Property_Type Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 18430\n",
      "Test size: 6966\n"
     ]
    }
   ],
   "source": [
    "#TENURE COLUMN\n",
    "mask_999 = ['947-year leasehold', '929-year leasehold', '946-year leasehold', '956-year leasehold']\n",
    "mask_99 = ['100-year leasehold', '102-year leasehold', '110-year leasehold', '103-year leasehold']\n",
    "df_cleaned = df_cleaned.replace(mask_999, '999-year leasehold')\n",
    "df_cleaned = df_cleaned.replace(mask_99, '99-year leasehold')\n",
    "\n",
    "df_test_cleaned = df_test_cleaned.replace(mask_999, '999-year leasehold')\n",
    "df_test_cleaned = df_test_cleaned.replace(mask_99, '99-year leasehold')\n",
    "\n",
    "print(f\"Train size: {df_cleaned.shape[0]}\")\n",
    "print(f\"Test size: {df_test_cleaned.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 18428\n",
      "Test size: 6966\n"
     ]
    }
   ],
   "source": [
    "#PROPERTY TYPE\n",
    "# changing \"hdb 3 rooms\", \"hdb 4 rooms\" and likewise to \"hdb\" since the number of rooms info can\n",
    "# be obtained from \"num_beds\"\n",
    "df_cleaned['property_type'].mask(df_cleaned['property_type'].str.contains(\"hdb\"), \"hdb\", inplace=True)\n",
    "df_cleaned.drop(df_cleaned[df_cleaned['property_type']  == 'land only'].index, inplace = True)\n",
    "\n",
    "df_test_cleaned['property_type'].mask(df_test_cleaned['property_type'].str.contains(\"hdb\"), \"hdb\", inplace=True)\n",
    "\n",
    "#TODO: Reduce number of property types with less properties. Maybe do an EDA and figure out best way to remove them. Perhaps club \n",
    "#them in different category.\n",
    "\n",
    "df_cleaned.loc[(df_cleaned['property_type']==\"apartment\") & \n",
    "       (df_cleaned['title'].str.contains('condo')),['property_type']] = \"condo\"\n",
    "\n",
    "df_cleaned.loc[(df_cleaned['property_type']==\"walk-up\") & \n",
    "       (df_cleaned['title'].str.contains('condo')),['property_type']] = \"condo\"\n",
    "\n",
    "df_cleaned.loc[(df_cleaned['property_type']==\"good class bungalow\"),['property_type']] = \"bungalow\"\n",
    "\n",
    "#Test\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"apartment\") & \n",
    "       (df_test_cleaned['title'].str.contains('condo')),['property_type']] = \"condo\"\n",
    "\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"walk-up\") & \n",
    "       (df_test_cleaned['title'].str.contains('condo')),['property_type']] = \"condo\"\n",
    "\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"good class bungalow\"),['property_type']] = \"bungalow\"\n",
    "\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"conservation house\") & \n",
    "       (df_test_cleaned['property_details_url'].str.contains('condo')),['property_type']] = \"condo\"\n",
    "\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"good class bungalow\"),['property_type']] = \"bungalow\"\n",
    "\n",
    "#The following results have been picked manually from the website 99.co\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"conservation house\") & \n",
    "       (df_test_cleaned['property_details_url'].str.contains('blair-plain-conservation-area')),['property_type']] = \"landed\"\n",
    "\n",
    "df_test_cleaned.loc[(df_test_cleaned['property_type']==\"conservation house\") & \n",
    "       (df_test_cleaned['property_details_url'].str.contains('beng-tong-mansion')),['property_type']] = \"landed\"\n",
    "\n",
    "print(f\"Train size: {df_cleaned.shape[0]}\")\n",
    "print(f\"Test size: {df_test_cleaned.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "28\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#h. Handle missing values\n",
    "\n",
    "#built_year\n",
    "dfmap = df_cleaned.dropna(subset = ['built_year'])[['built_year', 'property_name']].drop_duplicates()\n",
    "dfmap = dfmap.sort_values(['property_name', 'built_year']).drop_duplicates(subset = ['property_name'], keep='first')\n",
    "df_cleaned = df_cleaned.drop(columns=['built_year']).merge(dfmap, on=['property_name'], how='left')\n",
    "\n",
    "# fill nan built year of train dataset with median built year of the same subzone in the training dataset\n",
    "dftemp1 = df_train.groupby('subzone')['built_year'].median().reset_index(name='subzone_built_year')\n",
    "df_cleaned = df_cleaned.merge(dftemp1, how='left', on=['subzone'])\n",
    "\n",
    "mask = df_cleaned['built_year'].isnull()\n",
    "df_cleaned.loc[mask, 'built_year'] = df_cleaned.loc[mask, 'subzone_built_year']\n",
    "df_cleaned = df_cleaned.drop(columns=['subzone_built_year'])\n",
    "\n",
    "# check how many nans left in df_test\n",
    "print(df_cleaned['built_year'].isnull().sum())\n",
    "\n",
    "# fill nan built year of train dataset with median built year of the same subzone in the training dataset\n",
    "dftemp1 = df_train.groupby('planning_area')['built_year'].median().reset_index(name='planning_area_built_year')\n",
    "df_cleaned = df_cleaned.merge(dftemp1, how='left', on=['planning_area'])\n",
    "\n",
    "mask = df_cleaned['built_year'].isnull()\n",
    "df_cleaned.loc[mask, 'built_year'] = df_cleaned.loc[mask, 'planning_area_built_year']\n",
    "df_cleaned = df_cleaned.drop(columns=['planning_area_built_year'])\n",
    "\n",
    "# check how many nans left in df_test\n",
    "print(df_cleaned['built_year'].isnull().sum())\n",
    "\n",
    "df_cleaned['built_year'] = df_cleaned['built_year'].fillna(df_cleaned['built_year'].median())\n",
    "\n",
    "print(df_cleaned['built_year'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can see that there are still 20 properties without built year.\n",
      "['copen grand' 'jurong park' 'ponggol park']\n"
     ]
    }
   ],
   "source": [
    "# fill nan built year of test dataset with median built year of the same subzone in the training dataset\n",
    "dftemp1 = df_train.groupby('subzone')['built_year'].median().reset_index(name='subzone_built_year')\n",
    "df_test_cleaned = df_test_cleaned.merge(dftemp1, how='left', on=['subzone'])\n",
    "\n",
    "mask = df_test_cleaned['built_year'].isnull()\n",
    "df_test_cleaned.loc[mask, 'built_year'] = df_test_cleaned.loc[mask, 'subzone_built_year']\n",
    "df_test_cleaned = df_test_cleaned.drop(columns=['subzone_built_year'])\n",
    "\n",
    "# check how many nans left in df_test\n",
    "print(\"We can see that there are still 20 properties without built year.\")\n",
    "print(df_test_cleaned[df_test_cleaned['built_year'].isnull()]['property_name'].unique())\n",
    "\n",
    "# quick search of data online\n",
    "\n",
    "\n",
    "df_test_cleaned.loc[df_test_cleaned[\"property_name\"] == 'copen grand', 'built_year'] = 2027\n",
    "df_test_cleaned.loc[df_test_cleaned[\"property_name\"] == \"jurong park\", 'built_year'] = 1971\n",
    "df_test_cleaned.loc[df_test_cleaned[\"property_name\"] == \"ponggol park\", 'built_year'] = 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned1 = df_cleaned.copy()\n",
    "df_test_cleaned1 = df_test_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_beds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1669.0</td>\n",
       "      <td>540.603355</td>\n",
       "      <td>125.571278</td>\n",
       "      <td>129.0</td>\n",
       "      <td>474.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>560.00</td>\n",
       "      <td>1539.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>4146.0</td>\n",
       "      <td>778.088037</td>\n",
       "      <td>283.170470</td>\n",
       "      <td>65.0</td>\n",
       "      <td>667.0</td>\n",
       "      <td>721.0</td>\n",
       "      <td>807.00</td>\n",
       "      <td>8449.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>6947.0</td>\n",
       "      <td>1349.038146</td>\n",
       "      <td>14211.133365</td>\n",
       "      <td>92.0</td>\n",
       "      <td>990.0</td>\n",
       "      <td>1098.0</td>\n",
       "      <td>1249.00</td>\n",
       "      <td>1185000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>3170.0</td>\n",
       "      <td>2516.931230</td>\n",
       "      <td>26563.911725</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1313.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>2292.75</td>\n",
       "      <td>1496000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>1494.0</td>\n",
       "      <td>4144.005355</td>\n",
       "      <td>2283.154536</td>\n",
       "      <td>377.0</td>\n",
       "      <td>2045.0</td>\n",
       "      <td>3945.5</td>\n",
       "      <td>5661.00</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>713.0</td>\n",
       "      <td>6704.269285</td>\n",
       "      <td>4620.196437</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>4865.0</td>\n",
       "      <td>6337.0</td>\n",
       "      <td>7670.00</td>\n",
       "      <td>86080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>134.0</td>\n",
       "      <td>7231.559701</td>\n",
       "      <td>3187.888448</td>\n",
       "      <td>1851.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>6790.0</td>\n",
       "      <td>8970.00</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>8747.200000</td>\n",
       "      <td>4273.548902</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>5918.5</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>11500.00</td>\n",
       "      <td>27500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>12.0</td>\n",
       "      <td>7350.750000</td>\n",
       "      <td>2170.445875</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>6125.0</td>\n",
       "      <td>6600.0</td>\n",
       "      <td>9015.75</td>\n",
       "      <td>11515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>26.0</td>\n",
       "      <td>12035.269231</td>\n",
       "      <td>4780.932474</td>\n",
       "      <td>5285.0</td>\n",
       "      <td>8800.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>14250.00</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count          mean           std     min     25%      50%  \\\n",
       "num_beds                                                                \n",
       "1.0       1669.0    540.603355    125.571278   129.0   474.0    506.0   \n",
       "2.0       4146.0    778.088037    283.170470    65.0   667.0    721.0   \n",
       "3.0       6947.0   1349.038146  14211.133365    92.0   990.0   1098.0   \n",
       "4.0       3170.0   2516.931230  26563.911725   103.0  1313.0   1518.0   \n",
       "5.0       1494.0   4144.005355   2283.154536   377.0  2045.0   3945.5   \n",
       "6.0        713.0   6704.269285   4620.196437  1174.0  4865.0   6337.0   \n",
       "7.0        134.0   7231.559701   3187.888448  1851.0  4800.0   6790.0   \n",
       "8.0         40.0   8747.200000   4273.548902  3300.0  5918.5   8000.0   \n",
       "9.0         12.0   7350.750000   2170.445875  4800.0  6125.0   6600.0   \n",
       "10.0        26.0  12035.269231   4780.932474  5285.0  8800.0  12000.0   \n",
       "\n",
       "               75%        max  \n",
       "num_beds                       \n",
       "1.0         560.00     1539.0  \n",
       "2.0         807.00     8449.0  \n",
       "3.0        1249.00  1185000.0  \n",
       "4.0        2292.75  1496000.0  \n",
       "5.0        5661.00    20000.0  \n",
       "6.0        7670.00    86080.0  \n",
       "7.0        8970.00    20000.0  \n",
       "8.0       11500.00    27500.0  \n",
       "9.0        9015.75    11515.0  \n",
       "10.0      14250.00    30000.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned1.groupby('num_beds')['size_sqft'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#h. Handle missing values\n",
    "#print(df_test_cleaned.isnull().sum())\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "mask = df_cleaned1.num_baths.isnull()\n",
    "#Fill in number of baths from number of bedrooms.\n",
    "df_cleaned1.loc[mask & (df_cleaned1.num_beds == 1), ['num_baths']] = 1\n",
    "df_cleaned1.loc[mask & (df_cleaned1.num_beds == 2), ['num_baths']] = 2\n",
    "df_cleaned1.loc[mask & (df_cleaned1.num_beds > 2), ['num_baths']] = df_cleaned1.loc[mask & (df_cleaned1.num_beds > 2), 'num_beds'] - 1\n",
    "\n",
    "#Fill in number of beds from number of bathrooms.\n",
    "mask = df_cleaned1.num_beds.isnull()\n",
    "df_cleaned1.loc[mask & (df_cleaned1.num_baths == 1), ['num_beds']] = 1\n",
    "df_cleaned1.loc[mask & (df_cleaned1.num_baths == 2), ['num_beds']] = 2\n",
    "df_cleaned1.loc[mask & (df_cleaned1.num_baths > 2), ['num_beds']] = df_cleaned1.loc[mask & (df_cleaned1.num_baths > 2), 'num_baths'] + 1\n",
    "\n",
    "#There are 3 properties which have both Nan values for bed and baths. Estimate their beds and baths from area and quick search online\n",
    "df_cleaned1[df_cleaned1.num_beds.isnull()]\n",
    "#https://www.99.co/singapore/sale/property/camborne-road-landed-3TPbVtLAGHNX8kMDYqMV38\n",
    "df_cleaned1.loc[(df_cleaned1.property_name == 'dunearn estate') & mask, ['num_beds']] = 5\n",
    "df_cleaned1.loc[(df_cleaned1.property_name == 'dunearn estate') & mask, ['num_baths']] = 5\n",
    "\n",
    "df_cleaned1.loc[(df_cleaned1.property_name == 'one pearl bank') & mask, ['num_beds']] = 1\n",
    "df_cleaned1.loc[(df_cleaned1.property_name == 'one pearl bank') & mask, ['num_baths']] = 1\n",
    "\n",
    "df_cleaned1.loc[(df_cleaned1.property_name == 'gombak view') & mask, ['num_beds']] = 2\n",
    "df_cleaned1.loc[(df_cleaned1.property_name == 'gombak view') & mask, ['num_baths']] = 2\n",
    "\n",
    "df_cleaned = df_cleaned1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_test_cleaned1.num_baths.isnull()\n",
    "df_test_cleaned1.loc[mask & (df_test_cleaned1.num_beds == 1), ['num_baths']] = 1\n",
    "df_test_cleaned1.loc[mask & (df_test_cleaned1.num_beds == 2), ['num_baths']] = 2\n",
    "df_test_cleaned1.loc[mask & (df_test_cleaned1.num_beds > 2), ['num_baths']] = df_test_cleaned1.loc[mask & (df_test_cleaned1.num_beds > 2), 'num_beds'] - 1\n",
    "\n",
    "#Fill in number of beds from number of bathrooms.\n",
    "mask = df_test_cleaned1.num_beds.isnull()\n",
    "df_test_cleaned1.loc[mask & (df_test_cleaned1.num_baths == 1), ['num_beds']] = 1\n",
    "df_test_cleaned1.loc[mask & (df_test_cleaned1.num_baths == 2), ['num_beds']] = 2\n",
    "df_test_cleaned1.loc[mask & (df_test_cleaned1.num_baths > 2), ['num_beds']] = df_test_cleaned1.loc[mask & (df_test_cleaned1.num_baths > 2), 'num_baths'] + 1\n",
    "\n",
    "#There are 3 properties which have both Nan values for bed and baths. Estimate their beds and baths from area and quick search online\n",
    "df_cleaned1[df_cleaned1.num_beds.isnull()]\n",
    "\n",
    "#https://www.99.co/singapore/sale/property/48-strathmore-avenue-hdb-3pepL7Q8CQFJhbD53xx9w5\n",
    "df_test_cleaned1.loc[(df_test_cleaned1.property_name == 'forfar heights') & mask, ['num_beds']] = 3\n",
    "df_test_cleaned1.loc[(df_test_cleaned1.property_name == 'forfar heights') & mask, ['num_baths']] = 2\n",
    "\n",
    "df_test_cleaned1.loc[(df_test_cleaned1.property_name == 'the carrara') & mask, ['num_beds']] = 4\n",
    "df_test_cleaned1.loc[(df_test_cleaned1.property_name == 'the carrara') & mask, ['num_baths']] = 5\n",
    "\n",
    "#https://www.99.co/singapore/sale/property/170-bishan-street-13-hdb-G5to4naWPjPMQgRjT48wAi\n",
    "df_test_cleaned1.loc[(df_test_cleaned1.property_name == '170 bishan street 13') & mask, ['num_beds']] = 3\n",
    "df_test_cleaned1.loc[(df_test_cleaned1.property_name == '170 bishan street 13') & mask, ['num_baths']] = 2\n",
    "\n",
    "df_test_cleaned1.loc[(df_test_cleaned1.property_name == 'hdb-toa payoh') & mask, ['num_beds']] = 3\n",
    "df_test_cleaned1.loc[(df_test_cleaned1.property_name == 'hdb-toa payoh') & mask, ['num_baths']] = 2\n",
    "\n",
    "#https://www.99.co/singapore/sale/property/735-yishun-street-72-hdb-HJ4QUT6cTLghnmehNtmcT6\n",
    "df_test_cleaned1.loc[(df_test_cleaned1.property_name == 'nee soon central vista') & mask, ['num_beds']] = 2\n",
    "df_test_cleaned1.loc[(df_test_cleaned1.property_name == 'nee soon central vista') & mask, ['num_baths']] = 2\n",
    "\n",
    "df_test_cleaned = df_test_cleaned1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# get most frequent tenure per property_type and num_beds\n",
    "data = list(df_cleaned.groupby(['property_type', 'num_beds', 'tenure']).size().groupby(level=[0, 1]).idxmax().values)\n",
    "dfmap = pd.DataFrame(data, columns=['property_type', 'num_beds', 'train_tenure'])\n",
    "# fill nan test set tenures with the most frequent tenure in the training set for the corresponding property_type and num_beds\n",
    "df_test_cleaned = df_test_cleaned.merge(dfmap, on=['property_type', 'num_beds'], how='left')\n",
    "mask = df_test_cleaned['tenure'].isnull()\n",
    "df_test_cleaned.loc[mask, 'tenure'] = df_test_cleaned.loc[mask, 'train_tenure']\n",
    "df_test_cleaned = df_test_cleaned.drop(columns=['train_tenure'])\n",
    "print(df_test_cleaned['tenure'].isnull().sum())\n",
    "\n",
    "\n",
    "# for the remaining nan tenure\n",
    "df_test_cleaned.loc[df_test_cleaned['tenure'].isnull(), 'tenure'] = 'freehold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# there are only 2 records which appear to be incorrect as num_beds =1, whereas num_baths = 10 and price is > 3e8\n",
    "# hence we can drop these two records so that the results are not skewed\n",
    "\n",
    "df_cleaned = df_cleaned[df_cleaned['price']<3e8]\n",
    "df_cleaned = df_cleaned[df_cleaned['size_sqft']< 1e5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Singapore has the following latitude and longitude coordinates in its extreme ends:\n",
    "1. left-most (Tuas) :  1.30871,103.64287\n",
    "2. right-most (Changi) : 1.34538,104.00270\n",
    "3. top-most (Sembawang) : 1.46227,103.79487\n",
    "4. bottom-most (Bukit Merah) : 1.28762,103.82467\n",
    "\n",
    "\n",
    "#### Min latitude - 1.28762       Max latitude - 1.46227\n",
    "\n",
    "#### Min longitude - 103.64         Max longitude - 104.00\n",
    "\n",
    "But we can see that in the data, min longitude is -77.065364 and max latitude is 69.486768 which are out of the range of latitude and longitude values \n",
    "\n",
    "<img src=\"images/singapore-lat-long-map.jpeg\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is interesting to note that in all the records where latitude and longitude have incorrect coordinates, planning_area and subzone have missing values, this can also be verified by checking for count of missing values\n",
      "1 tessensohn road       97\n",
      "38 lorong 32 geylang     6\n",
      "5 jalan mutiara          5\n",
      "17 farrer drive          3\n",
      "15 farrer drive          2\n",
      "Name: address, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_max_lng = df_cleaned[df_cleaned.lng > 121.0]\n",
    "df_min_lng = df_cleaned[df_cleaned.lng < -77.0]\n",
    "df_max_lat = df_cleaned[df_cleaned.lat > 69.0]\n",
    "df_wrong_coordinates = pd.concat([df_max_lng, df_min_lng, df_max_lat])\n",
    "\n",
    "print(\"It is interesting to note that in all the records where latitude and longitude have incorrect coordinates, planning_area and subzone have missing values, this can also be verified by checking for count of missing values\")\n",
    "print(df_wrong_coordinates[\"address\"].value_counts())\n",
    "# coordinates are incorrect for 5 'address'\n",
    "\n",
    "\n",
    "# using the 'address' we can manually correct the latitude, longitude coordinates along with \n",
    "# filling of values for sub zone and planning_area\n",
    "\n",
    "df_cleaned.loc[df_cleaned.address == \"1 tessensohn road\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.3164313, 103.8575321, 'balestier', 'novena'\n",
    "df_cleaned.loc[df_cleaned.address == \"38 lorong 32 geylang\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.31262, 103.88686, 'aljunied', 'geylang'\n",
    "df_cleaned.loc[df_cleaned.address == \"5 jalan mutiara\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.29565, 103.82887, 'leonie hill', 'river valley'\n",
    "df_cleaned.loc[df_cleaned.address == \"17 farrer drive\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.313259, 103.806622, 'holland road', 'bukit timah'\n",
    "df_cleaned.loc[df_cleaned.address == \"15 farrer drive\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.313259, 103.806622, 'holland road', 'bukit timah'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is interesting to note that in all the records where latitude and longitude have incorrect coordinates, planning_area and subzone have missing values, this can also be verified by checking for count of missing values\n",
      "1 tessensohn road       29\n",
      "38 lorong 32 geylang     2\n",
      "5 jalan mutiara          1\n",
      "17 farrer drive          1\n",
      "Name: address, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_max_lng = df_test_cleaned[df_test_cleaned.lng > 121.0]\n",
    "df_min_lng = df_test_cleaned[df_test_cleaned.lng < -77.0]\n",
    "df_max_lat = df_test_cleaned[df_test_cleaned.lat > 69.0]\n",
    "df_wrong_coordinates = pd.concat([df_max_lng, df_min_lng, df_max_lat])\n",
    "\n",
    "print(\"It is interesting to note that in all the records where latitude and longitude have incorrect coordinates, planning_area and subzone have missing values, this can also be verified by checking for count of missing values\")\n",
    "print(df_wrong_coordinates[\"address\"].value_counts())\n",
    "# coordinates are incorrect for 5 'address'\n",
    "\n",
    "\n",
    "# using the 'address' we can manually correct the latitude, longitude coordinates along with \n",
    "# filling of values for sub zone and planning_area\n",
    "\n",
    "df_test_cleaned.loc[df_test_cleaned.address == \"38 lorong 32 geylang\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.31262, 103.88686, 'aljunied', 'geylang'\n",
    "df_test_cleaned.loc[df_test_cleaned.address == \"17 farrer drive\", \n",
    "               ['property_type', 'lat', 'lng', 'subzone', 'planning_area']] = 'condo', 1.313259, 103.806622, 'holland road', 'bukit timah'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1 tessensohn road', '5 jalan mutiara'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_cleaned[df_test_cleaned['subzone'].isnull()]['address'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "listing_id                 0\n",
       "title                      0\n",
       "address                    2\n",
       "property_name              0\n",
       "property_type              0\n",
       "tenure                     0\n",
       "built_year                 0\n",
       "num_beds                   0\n",
       "num_baths                  0\n",
       "size_sqft                  0\n",
       "floor_level             5810\n",
       "furnishing                 0\n",
       "available_unit_types     520\n",
       "total_num_units         1900\n",
       "property_details_url       0\n",
       "lat                        0\n",
       "lng                        0\n",
       "elevation                  0\n",
       "subzone                    0\n",
       "planning_area              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill nan subzones and planning areas on the basis of address and online lookup\n",
    "df_test_cleaned.loc[df_test_cleaned[\"address\"].str.contains(\"1 tessensohn road\", na=False), 'subzone'] = 'lavender'\n",
    "df_test_cleaned.loc[df_test_cleaned[\"address\"].str.contains(\"1 tessensohn road\", na=False), 'planning_area'] = 'kallang'\n",
    "\n",
    "df_test_cleaned.loc[df_test_cleaned[\"address\"].str.contains(\"38 lorong 32 geylang\", na=False), 'subzone'] = 'aljunied'\n",
    "df_test_cleaned.loc[df_test_cleaned[\"address\"].str.contains(\"38 lorong 32 geylang\", na=False), 'planning_area'] = 'geylang'\n",
    "\n",
    "df_test_cleaned.loc[df_test_cleaned[\"address\"].str.contains(\"17 farrer drive\", na=False), 'subzone'] = 'leonie hill'\n",
    "df_test_cleaned.loc[df_test_cleaned[\"address\"].str.contains(\"17 farrer drive\", na=False), 'planning_area'] = 'river valley'\n",
    "\n",
    "df_test_cleaned.loc[df_test_cleaned[\"address\"].str.contains(\"5 jalan mutiara\", na=False), 'subzone'] = 'central subzone'\n",
    "df_test_cleaned.loc[df_test_cleaned[\"address\"].str.contains(\"5 jalan mutiara\", na=False), 'planning_area'] = 'downtown core'\n",
    "\n",
    "df_test_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Merging With Auxiliary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_pri_sch = pd.read_csv('../data/auxiliary-data/sg-primary-schools.csv').drop([\"name\", \"subzone\", 'planning_area'], axis=1).to_numpy()\n",
    "pri_sch_coor = np.array([[radians(_) for _ in coor] for coor in df_pri_sch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_sec_sch = pd.read_csv('../data/auxiliary-data/sg-secondary-schools.csv').drop([\"name\", \"subzone\", 'planning_area'], axis=1).to_numpy()\n",
    "sec_sch_coor = np.array([[radians(_) for _ in coor] for coor in df_sec_sch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_commercial_centres = pd.read_csv('../data/auxiliary-data/sg-commerical-centres.csv').drop([\"name\", \"type\", \"subzone\", 'planning_area'], axis=1).to_numpy()\n",
    "commercial_centres_coor = np.array([[radians(_) for _ in coor] for coor in df_commercial_centres])\n",
    "\n",
    "df_shopping_malls = pd.read_csv('../data/auxiliary-data/sg-shopping-malls.csv').drop([\"name\", \"subzone\", 'planning_area'], axis=1).to_numpy()\n",
    "shopping_malls_coor = np.array([[radians(_) for _ in coor] for coor in df_shopping_malls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_mrt_station = pd.read_csv('../data/auxiliary-data/sg-mrt-stations.csv')\n",
    "df_mrt_station = df_mrt_station.drop([\"name\", \"lat\", \"lng\", 'planning_area', 'code', 'line', 'opening_year'], axis=1)\n",
    "mrt_station_cleaned = df_mrt_station.value_counts().to_frame(name=\"mrt_station\")\n",
    "\n",
    "mrt_station_coor = pd.read_csv('../data/auxiliary-data/sg-mrt-stations.csv').drop([\"name\", \"subzone\", 'planning_area', 'code', 'line', 'opening_year'], axis=1).to_numpy()\n",
    "mrt_station_coor = np.array([[radians(_) for _ in coor] for coor in mrt_station_coor])\n",
    "\n",
    "df_train_coor = df_cleaned[[\"lat\", \"lng\"]].to_numpy()\n",
    "df_train_coor = np.array([[radians(_) for _ in coor] for coor in df_train_coor])\n",
    "\n",
    "df_test_coor = df_test_cleaned[[\"lat\", \"lng\"]].to_numpy()\n",
    "df_test_coor = np.array([[radians(_) for _ in coor] for coor in df_test_coor])\n",
    "\n",
    "dist_matrix_train_mrt = sklearn.metrics.pairwise.haversine_distances(mrt_station_coor, df_train_coor)\n",
    "# multiply to get meters\n",
    "closest_dist_to_mrt_train = pd.DataFrame(np.amin(dist_matrix_train_mrt, axis=0)* 6371000, columns=[\"closest_dist_to_mrt\"])\n",
    "\n",
    "dist_matrix_test_mrt = sklearn.metrics.pairwise.haversine_distances(mrt_station_coor, df_test_coor)\n",
    "# multiply to get meters\n",
    "closest_dist_to_mrt_test = pd.DataFrame(np.amin(dist_matrix_test_mrt, axis=0)* 6371000, columns=[\"closest_dist_to_mrt\"])\n",
    "\n",
    "dist_matrix_train_pri = sklearn.metrics.pairwise.haversine_distances(pri_sch_coor, df_train_coor)\n",
    "closest_dist_to_pri_train = pd.DataFrame(np.amin(dist_matrix_train_pri, axis=0)* 6371000, columns=[\"closest_dist_to_pri\"])\n",
    "\n",
    "dist_matrix_test_pri = sklearn.metrics.pairwise.haversine_distances(pri_sch_coor, df_test_coor)\n",
    "closest_dist_to_pri_test = pd.DataFrame(np.amin(dist_matrix_test_pri, axis=0)* 6371000, columns=[\"closest_dist_to_pri\"])\n",
    "\n",
    "dist_matrix_train_sec = sklearn.metrics.pairwise.haversine_distances(sec_sch_coor, df_train_coor)\n",
    "closest_dist_to_sec_train = pd.DataFrame(np.amin(dist_matrix_train_sec, axis=0)* 6371000, columns=[\"closest_dist_to_sec\"])\n",
    "\n",
    "dist_matrix_test_sec = sklearn.metrics.pairwise.haversine_distances(sec_sch_coor, df_test_coor)\n",
    "closest_dist_to_sec_test = pd.DataFrame(np.amin(dist_matrix_test_sec, axis=0)* 6371000, columns=[\"closest_dist_to_sec\"])\n",
    "\n",
    "dist_matrix_test_com = sklearn.metrics.pairwise.haversine_distances(commercial_centres_coor, df_test_coor)\n",
    "closest_dist_to_com_test = pd.DataFrame(np.amin(dist_matrix_test_com, axis=0)* 6371000, columns=[\"closest_dist_to_com\"])\n",
    "\n",
    "dist_matrix_test_shop = sklearn.metrics.pairwise.haversine_distances(shopping_malls_coor, df_test_coor)\n",
    "closest_dist_to_shop_test = pd.DataFrame(np.amin(dist_matrix_test_shop, axis=0)* 6371000, columns=[\"closest_dist_to_shop\"])\n",
    "\n",
    "dist_matrix_train_com = sklearn.metrics.pairwise.haversine_distances(commercial_centres_coor, df_train_coor)\n",
    "closest_dist_to_com_train = pd.DataFrame(np.amin(dist_matrix_train_com, axis=0)* 6371000, columns=[\"closest_dist_to_com\"])\n",
    "\n",
    "dist_matrix_train_shop = sklearn.metrics.pairwise.haversine_distances(shopping_malls_coor, df_train_coor)\n",
    "closest_dist_to_shop_train = pd.DataFrame(np.amin(dist_matrix_train_shop, axis=0)* 6371000, columns=[\"closest_dist_to_shop\"])\n",
    "\n",
    "#during pri sch registration exercise, homeowners within 1km will be given priority\n",
    "test = pd.DataFrame(dist_matrix_test_pri * 6371000)\n",
    "test_near_pri_sch = test[test <= 1000].count().rename(\"close_pri_sch\")\n",
    "\n",
    "test = pd.DataFrame(dist_matrix_train_pri * 6371000)\n",
    "train_near_pri_sch = test[test <= 1000].count().rename(\"close_pri_sch\")\n",
    "\n",
    "#no such priority for sec sch, put as 1km for now\n",
    "#change 1000 to desired distance if necessary\n",
    "test = pd.DataFrame(dist_matrix_test_sec * 6371000)\n",
    "test_near_sec_sch = test[test <= 1000].count().rename(\"close_sec_sch\")\n",
    "\n",
    "test = pd.DataFrame(dist_matrix_train_sec * 6371000)\n",
    "train_near_sec_sch = test[test <= 1000].count().rename(\"close_sec_sch\")\n",
    "\n",
    "df_cleaned = df_cleaned.merge(pri_sch_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(sec_sch_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(mrt_station_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(df_subzone, how='left',left_on=\"subzone\",right_on=\"name\")\\\n",
    "    .join(closest_dist_to_mrt_train)\\\n",
    "    .join(closest_dist_to_pri_train)\\\n",
    "    .join(closest_dist_to_sec_train)\\\n",
    "    .join(train_near_pri_sch)\\\n",
    "    .join(train_near_sec_sch)\\\n",
    "    .join(closest_dist_to_shop_train)\\\n",
    "    .join(closest_dist_to_com_train)\\\n",
    "    .fillna({'pri_sch':0, 'sec_sch':0, 'mrt_station':0})\n",
    "\n",
    "df_test_cleaned = df_test_cleaned.merge(pri_sch_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(sec_sch_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(mrt_station_cleaned, how='left',left_on=\"subzone\",right_on=\"subzone\")\\\n",
    "    .merge(df_subzone, how='left',left_on=\"subzone\",right_on=\"name\")\\\n",
    "    .join(closest_dist_to_mrt_test)\\\n",
    "    .join(closest_dist_to_pri_test)\\\n",
    "    .join(closest_dist_to_sec_test)\\\n",
    "    .join(test_near_pri_sch)\\\n",
    "    .join(test_near_sec_sch)\\\n",
    "    .join(closest_dist_to_shop_test)\\\n",
    "    .join(closest_dist_to_com_test)\\\n",
    "    .fillna({'pri_sch':0, 'sec_sch':0, 'mrt_station':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furnishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['unfurnished', 'unspecified', 'fully', 'partial'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_cleaned['furnishing'].replace('na', 'unspecified', inplace = True)\n",
    "df_test_cleaned['furnishing'].unique()\n",
    "\n",
    "df_test_cleaned['furnishing'].replace('na', 'unspecified', inplace = True)\n",
    "df_test_cleaned['furnishing'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers in Price Per SQFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flooring and capping price per sqft at 1% and 99%ile respectively\n",
    "df_cleaned[\"price_per_sqft\"] = df_cleaned[\"price\"]/df_cleaned[\"size_sqft\"]\n",
    "low, high = df_cleaned['price_per_sqft'].quantile([0.01, 0.99])\n",
    "df_cleaned['price_per_sqft'] = np.clip(df_cleaned['price_per_sqft'], low, high)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Drop Columns at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.drop(['title', 'address','property_name', 'property_details_url', 'listing_id', 'elevation', 'total_num_units', 'floor_level', 'available_unit_types'], axis = 1)\n",
    "df_test_cleaned = df_test_cleaned.drop(['title','address','property_name', 'property_details_url', 'listing_id', 'elevation', 'total_num_units', 'floor_level', 'available_unit_types'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "(18424, 24)\n",
      "(6966, 22)\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned.isnull().sum().sum())\n",
    "print(df_test_cleaned.isnull().sum().sum())\n",
    "print(df_cleaned.shape)\n",
    "print(df_test_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_test_cleaned.to_csv('../data/test_cleaned.csv', index = False)\n",
    "df_cleaned.to_csv('../data/train_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
