{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/32/zv3nm8_12xb9ytd7vk7nnr6w0000gn/T/ipykernel_87566/4280354733.py:8: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from pandas import MultiIndex, Int64Index\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import autogluon.core as ag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna\n",
    "# !pip install scipy\n",
    "# !pip install autogluon\n",
    "# !pip install xgboost\n",
    "# !python3.9 -m pip install --upgrade pip\n",
    "# !pip install bokeh==2.0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train_encoded_pca_cum.csv')\n",
    "df_test = pd.read_csv('../data/test_encoded_pca_cum.csv')\n",
    "#train_encoded_pca_cum\n",
    "#train_encoded_pca\n",
    "#ex = pd.read_csv('../data/example-submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_sqft</th>\n",
       "      <th>property_type_cluster house</th>\n",
       "      <th>property_type_condo</th>\n",
       "      <th>property_type_corner terrace</th>\n",
       "      <th>property_type_executive condo</th>\n",
       "      <th>property_type_hdb</th>\n",
       "      <th>property_type_semi-detached house</th>\n",
       "      <th>property_type_terraced house</th>\n",
       "      <th>property_type_infrequent_sklearn</th>\n",
       "      <th>furnishing_partial</th>\n",
       "      <th>...</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "      <th>PC11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>463</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034716</td>\n",
       "      <td>-0.942867</td>\n",
       "      <td>-0.870046</td>\n",
       "      <td>0.544891</td>\n",
       "      <td>0.442697</td>\n",
       "      <td>1.227460</td>\n",
       "      <td>-0.261023</td>\n",
       "      <td>-0.960449</td>\n",
       "      <td>0.191969</td>\n",
       "      <td>-0.113819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1033</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.369292</td>\n",
       "      <td>-1.030715</td>\n",
       "      <td>-1.349437</td>\n",
       "      <td>-1.661337</td>\n",
       "      <td>0.821686</td>\n",
       "      <td>-0.620200</td>\n",
       "      <td>0.470905</td>\n",
       "      <td>0.296070</td>\n",
       "      <td>0.190211</td>\n",
       "      <td>-0.479004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457908</td>\n",
       "      <td>1.409650</td>\n",
       "      <td>0.053265</td>\n",
       "      <td>-0.590787</td>\n",
       "      <td>-0.330942</td>\n",
       "      <td>0.744301</td>\n",
       "      <td>0.286882</td>\n",
       "      <td>0.111819</td>\n",
       "      <td>-0.054269</td>\n",
       "      <td>0.449271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1216</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.276300</td>\n",
       "      <td>-0.634599</td>\n",
       "      <td>-0.234421</td>\n",
       "      <td>0.553224</td>\n",
       "      <td>-1.217748</td>\n",
       "      <td>-1.105000</td>\n",
       "      <td>2.080035</td>\n",
       "      <td>-1.147152</td>\n",
       "      <td>-0.277603</td>\n",
       "      <td>0.817754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>936</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.797542</td>\n",
       "      <td>1.959214</td>\n",
       "      <td>-1.115832</td>\n",
       "      <td>0.824612</td>\n",
       "      <td>-0.043025</td>\n",
       "      <td>1.433574</td>\n",
       "      <td>-0.593093</td>\n",
       "      <td>-1.003330</td>\n",
       "      <td>0.771620</td>\n",
       "      <td>0.757636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   size_sqft  property_type_cluster house  property_type_condo  \\\n",
       "0        463                            0                    1   \n",
       "1       1033                            0                    1   \n",
       "2        570                            0                    1   \n",
       "3       1216                            0                    0   \n",
       "4        936                            0                    0   \n",
       "\n",
       "   property_type_corner terrace  property_type_executive condo  \\\n",
       "0                             0                              0   \n",
       "1                             0                              0   \n",
       "2                             0                              0   \n",
       "3                             0                              0   \n",
       "4                             0                              0   \n",
       "\n",
       "   property_type_hdb  property_type_semi-detached house  \\\n",
       "0                  0                                  0   \n",
       "1                  0                                  0   \n",
       "2                  0                                  0   \n",
       "3                  1                                  0   \n",
       "4                  1                                  0   \n",
       "\n",
       "   property_type_terraced house  property_type_infrequent_sklearn  \\\n",
       "0                             0                                 0   \n",
       "1                             0                                 0   \n",
       "2                             0                                 0   \n",
       "3                             0                                 0   \n",
       "4                             0                                 0   \n",
       "\n",
       "   furnishing_partial  ...       PC2       PC3       PC4       PC5       PC6  \\\n",
       "0                   0  ...  0.034716 -0.942867 -0.870046  0.544891  0.442697   \n",
       "1                   0  ...  0.369292 -1.030715 -1.349437 -1.661337  0.821686   \n",
       "2                   0  ...  0.457908  1.409650  0.053265 -0.590787 -0.330942   \n",
       "3                   0  ... -0.276300 -0.634599 -0.234421  0.553224 -1.217748   \n",
       "4                   0  ...  1.797542  1.959214 -1.115832  0.824612 -0.043025   \n",
       "\n",
       "        PC7       PC8       PC9      PC10      PC11  \n",
       "0  1.227460 -0.261023 -0.960449  0.191969 -0.113819  \n",
       "1 -0.620200  0.470905  0.296070  0.190211 -0.479004  \n",
       "2  0.744301  0.286882  0.111819 -0.054269  0.449271  \n",
       "3 -1.105000  2.080035 -1.147152 -0.277603  0.817754  \n",
       "4  1.433574 -0.593093 -1.003330  0.771620  0.757636  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, valid = train_test_split(df_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train.drop(['price'], axis=1)\n",
    "\n",
    "X_valid = valid.drop(['price'], axis=1)\n",
    "y_valid = valid['price_per_sqft']\n",
    "\n",
    "df_test_size_sqft = df_test['size_sqft'].copy()\n",
    "#df_test= df_test.drop(['size_sqft'], axis=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "idxs = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(idxs)\n",
    "df_test.columns.difference(X_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_sqft</th>\n",
       "      <th>property_type_cluster house</th>\n",
       "      <th>property_type_condo</th>\n",
       "      <th>property_type_corner terrace</th>\n",
       "      <th>property_type_executive condo</th>\n",
       "      <th>property_type_hdb</th>\n",
       "      <th>property_type_semi-detached house</th>\n",
       "      <th>property_type_terraced house</th>\n",
       "      <th>property_type_infrequent_sklearn</th>\n",
       "      <th>furnishing_partial</th>\n",
       "      <th>...</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "      <th>PC11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>463</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034716</td>\n",
       "      <td>-0.942867</td>\n",
       "      <td>-0.870046</td>\n",
       "      <td>0.544891</td>\n",
       "      <td>0.442697</td>\n",
       "      <td>1.227460</td>\n",
       "      <td>-0.261023</td>\n",
       "      <td>-0.960449</td>\n",
       "      <td>0.191969</td>\n",
       "      <td>-0.113819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1033</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.369292</td>\n",
       "      <td>-1.030715</td>\n",
       "      <td>-1.349437</td>\n",
       "      <td>-1.661337</td>\n",
       "      <td>0.821686</td>\n",
       "      <td>-0.620200</td>\n",
       "      <td>0.470905</td>\n",
       "      <td>0.296070</td>\n",
       "      <td>0.190211</td>\n",
       "      <td>-0.479004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457908</td>\n",
       "      <td>1.409650</td>\n",
       "      <td>0.053265</td>\n",
       "      <td>-0.590787</td>\n",
       "      <td>-0.330942</td>\n",
       "      <td>0.744301</td>\n",
       "      <td>0.286882</td>\n",
       "      <td>0.111819</td>\n",
       "      <td>-0.054269</td>\n",
       "      <td>0.449271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1216</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.276300</td>\n",
       "      <td>-0.634599</td>\n",
       "      <td>-0.234421</td>\n",
       "      <td>0.553224</td>\n",
       "      <td>-1.217748</td>\n",
       "      <td>-1.105000</td>\n",
       "      <td>2.080035</td>\n",
       "      <td>-1.147152</td>\n",
       "      <td>-0.277603</td>\n",
       "      <td>0.817754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>936</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.797542</td>\n",
       "      <td>1.959214</td>\n",
       "      <td>-1.115832</td>\n",
       "      <td>0.824612</td>\n",
       "      <td>-0.043025</td>\n",
       "      <td>1.433574</td>\n",
       "      <td>-0.593093</td>\n",
       "      <td>-1.003330</td>\n",
       "      <td>0.771620</td>\n",
       "      <td>0.757636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   size_sqft  property_type_cluster house  property_type_condo  \\\n",
       "0        463                            0                    1   \n",
       "1       1033                            0                    1   \n",
       "2        570                            0                    1   \n",
       "3       1216                            0                    0   \n",
       "4        936                            0                    0   \n",
       "\n",
       "   property_type_corner terrace  property_type_executive condo  \\\n",
       "0                             0                              0   \n",
       "1                             0                              0   \n",
       "2                             0                              0   \n",
       "3                             0                              0   \n",
       "4                             0                              0   \n",
       "\n",
       "   property_type_hdb  property_type_semi-detached house  \\\n",
       "0                  0                                  0   \n",
       "1                  0                                  0   \n",
       "2                  0                                  0   \n",
       "3                  1                                  0   \n",
       "4                  1                                  0   \n",
       "\n",
       "   property_type_terraced house  property_type_infrequent_sklearn  \\\n",
       "0                             0                                 0   \n",
       "1                             0                                 0   \n",
       "2                             0                                 0   \n",
       "3                             0                                 0   \n",
       "4                             0                                 0   \n",
       "\n",
       "   furnishing_partial  ...       PC2       PC3       PC4       PC5       PC6  \\\n",
       "0                   0  ...  0.034716 -0.942867 -0.870046  0.544891  0.442697   \n",
       "1                   0  ...  0.369292 -1.030715 -1.349437 -1.661337  0.821686   \n",
       "2                   0  ...  0.457908  1.409650  0.053265 -0.590787 -0.330942   \n",
       "3                   0  ... -0.276300 -0.634599 -0.234421  0.553224 -1.217748   \n",
       "4                   0  ...  1.797542  1.959214 -1.115832  0.824612 -0.043025   \n",
       "\n",
       "        PC7       PC8       PC9      PC10      PC11  \n",
       "0  1.227460 -0.261023 -0.960449  0.191969 -0.113819  \n",
       "1 -0.620200  0.470905  0.296070  0.190211 -0.479004  \n",
       "2  0.744301  0.286882  0.111819 -0.054269  0.449271  \n",
       "3 -1.105000  2.080035 -1.147152 -0.277603  0.817754  \n",
       "4  1.433574 -0.593093 -1.003330  0.771620  0.757636  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns.difference(df_test.columns)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = preprocessing.MinMaxScaler().fit(X_train)\n",
    "\n",
    "# xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42, eval_metric=[\"error\"])\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict labels for test data\n",
    "# yt_pred = xgb_model.predict(X_train)\n",
    "# yv_pred = xgb_model.predict(X_valid)\n",
    "\n",
    "# print('Training RMSE: {:.3f}'.format(mean_squared_error(yt_pred, y_train, squared=False)))\n",
    "# print('Test RMSE: {:.3f}'.format(mean_squared_error(yv_pred, y_valid, squared=False)))\n",
    "\n",
    "# print(f'Percentage error for training data {((abs(yt_pred - y_train)/y_train)*100).mean()}')\n",
    "# print(f'Percentage error for validation data {((abs(yv_pred - y_valid)/y_valid)*100).mean()}')\n",
    "\n",
    "# xgb.plot_importance(xgb_model, height = 5, max_num_features=20)\n",
    "\n",
    "# score = cross_val_score(xgb_model, X_train, y_train, n_jobs=-1, cv=5, scoring=\"r2\")\n",
    "# r2_mean = score.mean()\n",
    "\n",
    "# print(f\"R2 mean: {r2_mean}\")\n",
    "#yTest = xgb_model.predict(df_test)\n",
    "\n",
    "# yTest = yTest * df_test_size_sqft\n",
    "# yTest = pd.DataFrame(yTest.values, columns=['Predicted'], index=df_test.index)\n",
    "\n",
    "# yTest.to_csv('../predictions/xgboost.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20221019_103622/\"\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20221019_103622/\"\n",
      "AutoGluon Version:  0.5.2\n",
      "Python Version:     3.9.15\n",
      "Operating System:   Darwin\n",
      "Train Data Rows:    13773\n",
      "Train Data Columns: 64\n",
      "Tuning Data Rows:    3444\n",
      "Tuning Data Columns: 64\n",
      "Label Column: price_per_sqft\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (44625.0, 2.032620320855615, 1828.14799, 1223.50946)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "/usr/local/lib/python3.9/site-packages/autogluon/tabular/learner/default_learner.py:151: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  missinglabel_inds = [index for index, x in X[self.label].isna().iteritems() if x]\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5797.45 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.82 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 51 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 12 | ['PC0', 'PC1', 'PC2', 'PC3', 'PC4', ...]\n",
      "\t\t('int', [])   : 52 | ['size_sqft', 'property_type_cluster house', 'property_type_condo', 'property_type_corner terrace', 'property_type_executive condo', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 12 | ['PC0', 'PC1', 'PC2', 'PC3', 'PC4', ...]\n",
      "\t\t('int', [])       :  1 | ['size_sqft']\n",
      "\t\t('int', ['bool']) : 51 | ['property_type_cluster house', 'property_type_condo', 'property_type_corner terrace', 'property_type_executive condo', 'property_type_hdb', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t64 features in original data used to generate 64 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.67 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.38s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Excluded Model Types: ['NN_TORCH', 'FASTAI', 'KNN', 'CAT', 'RF']\n",
      "\tFound 'RF' model in hyperparameters, but 'RF' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'RF' model in hyperparameters, but 'RF' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'RF' model in hyperparameters, but 'RF' is present in `excluded_model_types` and will be removed.\n",
      "Fitting 10 L1 models ...\n",
      "Hyperparameter tuning model: LightGBM ... Tuning model for up to 80.97s of the 899.61s of remaining time.\n",
      "100%|██████████| 5/5 [00:46<00:00,  9.23s/it]\n",
      "Fitted model: LightGBM/T1 ...\n",
      "\t-204.4617\t = Validation score   (-mean_absolute_error)\n",
      "\t11.5s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitted model: LightGBM/T2 ...\n",
      "\t-196.438\t = Validation score   (-mean_absolute_error)\n",
      "\t8.26s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitted model: LightGBM/T3 ...\n",
      "\t-191.3946\t = Validation score   (-mean_absolute_error)\n",
      "\t9.48s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitted model: LightGBM/T4 ...\n",
      "\t-550.3736\t = Validation score   (-mean_absolute_error)\n",
      "\t7.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitted model: LightGBM/T5 ...\n",
      "\t-212.6703\t = Validation score   (-mean_absolute_error)\n",
      "\t9.26s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Hyperparameter tuning model: ExtraTreesEntr ... Tuning model for up to 80.97s of the 853.23s of remaining time.\n",
      "\tNo hyperparameter search space specified for ExtraTreesEntr. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "Warning: Exception caused ExtraTreesEntr to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1015, in _hyperparameter_tune\n",
      "    hpo_executor.validate_search_space(search_space, self.name)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/autogluon/core/hpo/executors.py\", line 335, in validate_search_space\n",
      "    raise EmptySearchSpace\n",
      "autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1388, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1001, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1017, in _hyperparameter_tune\n",
      "    return skip_hpo(self, X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/model_trial.py\", line 108, in skip_hpo\n",
      "    fit_and_save_model(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/model_trial.py\", line 83, in fit_and_save_model\n",
      "    model.fit(**fit_args, time_limit=time_left)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 189, in _fit\n",
      "    model = model.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/Users/nishita/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py\", line 1098, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/Users/nishita/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py\", line 975, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.15/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 771, in get\n",
      "    raise self._value\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.15/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/nishita/Library/Python/3.9/lib/python/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/nishita/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/nishita/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 1315, in fit\n",
      "    super().fit(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 356, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_, n_samples)\n",
      "KeyError: 'entropy'\n",
      "'entropy'\n",
      "Hyperparameter tuning model: ExtraTreesMSE ... Tuning model for up to 80.97s of the 852.97s of remaining time.\n",
      "\tNo hyperparameter search space specified for ExtraTreesMSE. Skipping HPO. Will train one model based on the provided hyperparameters.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [84], line 58\u001B[0m\n\u001B[1;32m     49\u001B[0m hyperparameter_tune_kwargs \u001B[38;5;241m=\u001B[39m {  \u001B[38;5;66;03m# HPO is not performed unless hyperparameter_tune_kwargs is specified\u001B[39;00m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_trials\u001B[39m\u001B[38;5;124m'\u001B[39m: num_trials,\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscheduler\u001B[39m\u001B[38;5;124m'\u001B[39m : \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msearcher\u001B[39m\u001B[38;5;124m'\u001B[39m: search_strategy,\n\u001B[1;32m     53\u001B[0m }\n\u001B[1;32m     57\u001B[0m excluded_model_types \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNN_TORCH\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFASTAI\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mKNN\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCAT\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRF\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 58\u001B[0m predictor_reg \u001B[38;5;241m=\u001B[39m \u001B[43mpredictor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data_reg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexcluded_model_types\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mexcluded_model_types\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtime_limit\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m900\u001B[39;49m\n\u001B[1;32m     59\u001B[0m \u001B[43m,\u001B[49m\u001B[43mhyperparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhyperparameters\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m,\u001B[49m\u001B[43mtuning_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalid_data_reg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhyperparameter_tune_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhyperparameter_tune_kwargs\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/autogluon/core/utils/decorators.py:30\u001B[0m, in \u001B[0;36munpack.<locals>._unpack_inner.<locals>._call\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[39m@functools\u001B[39m\u001B[39m.\u001B[39mwraps(f)\n\u001B[1;32m     28\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_call\u001B[39m(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs):\n\u001B[1;32m     29\u001B[0m     gargs, gkwargs \u001B[39m=\u001B[39m g(\u001B[39m*\u001B[39mother_args, \u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)\n\u001B[0;32m---> 30\u001B[0m     \u001B[39mreturn\u001B[39;00m f(\u001B[39m*\u001B[39;49mgargs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mgkwargs)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py:831\u001B[0m, in \u001B[0;36mTabularPredictor.fit\u001B[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, **kwargs)\u001B[0m\n\u001B[1;32m    823\u001B[0m core_kwargs \u001B[39m=\u001B[39m {\n\u001B[1;32m    824\u001B[0m     \u001B[39m'\u001B[39m\u001B[39mag_args\u001B[39m\u001B[39m'\u001B[39m: ag_args,\n\u001B[1;32m    825\u001B[0m     \u001B[39m'\u001B[39m\u001B[39mag_args_ensemble\u001B[39m\u001B[39m'\u001B[39m: ag_args_ensemble,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    828\u001B[0m     \u001B[39m'\u001B[39m\u001B[39mfeature_prune_kwargs\u001B[39m\u001B[39m'\u001B[39m: kwargs\u001B[39m.\u001B[39mget(\u001B[39m'\u001B[39m\u001B[39mfeature_prune_kwargs\u001B[39m\u001B[39m'\u001B[39m, \u001B[39mNone\u001B[39;00m)\n\u001B[1;32m    829\u001B[0m }\n\u001B[1;32m    830\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39msave(silent\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m)  \u001B[39m# Save predictor to disk to enable prediction and training after interrupt\u001B[39;00m\n\u001B[0;32m--> 831\u001B[0m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_learner\u001B[39m.\u001B[39;49mfit(X\u001B[39m=\u001B[39;49mtrain_data, X_val\u001B[39m=\u001B[39;49mtuning_data, X_unlabeled\u001B[39m=\u001B[39;49munlabeled_data,\n\u001B[1;32m    832\u001B[0m                   holdout_frac\u001B[39m=\u001B[39;49mholdout_frac, num_bag_folds\u001B[39m=\u001B[39;49mnum_bag_folds, num_bag_sets\u001B[39m=\u001B[39;49mnum_bag_sets,\n\u001B[1;32m    833\u001B[0m                   num_stack_levels\u001B[39m=\u001B[39;49mnum_stack_levels,\n\u001B[1;32m    834\u001B[0m                   hyperparameters\u001B[39m=\u001B[39;49mhyperparameters, core_kwargs\u001B[39m=\u001B[39;49mcore_kwargs,\n\u001B[1;32m    835\u001B[0m                   time_limit\u001B[39m=\u001B[39;49mtime_limit, infer_limit\u001B[39m=\u001B[39;49minfer_limit, infer_limit_batch_size\u001B[39m=\u001B[39;49minfer_limit_batch_size,\n\u001B[1;32m    836\u001B[0m                   verbosity\u001B[39m=\u001B[39;49mverbosity, use_bag_holdout\u001B[39m=\u001B[39;49muse_bag_holdout)\n\u001B[1;32m    837\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_set_post_fit_vars()\n\u001B[1;32m    839\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_post_fit(\n\u001B[1;32m    840\u001B[0m     keep_only_best\u001B[39m=\u001B[39mkwargs[\u001B[39m'\u001B[39m\u001B[39mkeep_only_best\u001B[39m\u001B[39m'\u001B[39m],\n\u001B[1;32m    841\u001B[0m     refit_full\u001B[39m=\u001B[39mkwargs[\u001B[39m'\u001B[39m\u001B[39mrefit_full\u001B[39m\u001B[39m'\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    845\u001B[0m     infer_limit\u001B[39m=\u001B[39minfer_limit,\n\u001B[1;32m    846\u001B[0m )\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/autogluon/tabular/learner/abstract_learner.py:118\u001B[0m, in \u001B[0;36mAbstractTabularLearner.fit\u001B[0;34m(self, X, X_val, **kwargs)\u001B[0m\n\u001B[1;32m    116\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mAssertionError\u001B[39;00m(\u001B[39m'\u001B[39m\u001B[39mLearner is already fit.\u001B[39m\u001B[39m'\u001B[39m)\n\u001B[1;32m    117\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_validate_fit_input(X\u001B[39m=\u001B[39mX, X_val\u001B[39m=\u001B[39mX_val, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)\n\u001B[0;32m--> 118\u001B[0m \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_fit(X\u001B[39m=\u001B[39;49mX, X_val\u001B[39m=\u001B[39;49mX_val, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/autogluon/tabular/learner/default_learner.py:126\u001B[0m, in \u001B[0;36mDefaultLearner._fit\u001B[0;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001B[0m\n\u001B[1;32m    123\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39meval_metric \u001B[39m=\u001B[39m trainer\u001B[39m.\u001B[39meval_metric\n\u001B[1;32m    125\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39msave()\n\u001B[0;32m--> 126\u001B[0m trainer\u001B[39m.\u001B[39;49mfit(\n\u001B[1;32m    127\u001B[0m     X\u001B[39m=\u001B[39;49mX,\n\u001B[1;32m    128\u001B[0m     y\u001B[39m=\u001B[39;49my,\n\u001B[1;32m    129\u001B[0m     X_val\u001B[39m=\u001B[39;49mX_val,\n\u001B[1;32m    130\u001B[0m     y_val\u001B[39m=\u001B[39;49my_val,\n\u001B[1;32m    131\u001B[0m     X_unlabeled\u001B[39m=\u001B[39;49mX_unlabeled,\n\u001B[1;32m    132\u001B[0m     holdout_frac\u001B[39m=\u001B[39;49mholdout_frac,\n\u001B[1;32m    133\u001B[0m     time_limit\u001B[39m=\u001B[39;49mtime_limit_trainer,\n\u001B[1;32m    134\u001B[0m     infer_limit\u001B[39m=\u001B[39;49minfer_limit,\n\u001B[1;32m    135\u001B[0m     infer_limit_batch_size\u001B[39m=\u001B[39;49minfer_limit_batch_size,\n\u001B[1;32m    136\u001B[0m     groups\u001B[39m=\u001B[39;49mgroups,\n\u001B[1;32m    137\u001B[0m     \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mtrainer_fit_kwargs\n\u001B[1;32m    138\u001B[0m )\n\u001B[1;32m    139\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39msave_trainer(trainer\u001B[39m=\u001B[39mtrainer)\n\u001B[1;32m    140\u001B[0m time_end \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime()\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/autogluon/tabular/trainer/auto_trainer.py:85\u001B[0m, in \u001B[0;36mAutoTrainer.fit\u001B[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001B[0m\n\u001B[1;32m     76\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m use_bag_holdout:\n\u001B[1;32m     77\u001B[0m         \u001B[39m# TODO: User could be intending to blend instead. Add support for blend stacking.\u001B[39;00m\n\u001B[1;32m     78\u001B[0m         \u001B[39m#  This error message is necessary because when calculating out-of-fold predictions for user, we want to return them in the form given in train_data,\u001B[39;00m\n\u001B[1;32m     79\u001B[0m         \u001B[39m#  but if we merge train and val here, it becomes very confusing from a users perspective, especially because we reset index, making it impossible to match\u001B[39;00m\n\u001B[1;32m     80\u001B[0m         \u001B[39m#  the original train_data to the out-of-fold predictions from `predictor.get_oof_pred_proba()`.\u001B[39;00m\n\u001B[1;32m     81\u001B[0m         \u001B[39mraise\u001B[39;00m \u001B[39mAssertionError\u001B[39;00m(\u001B[39m'\u001B[39m\u001B[39mX_val, y_val is not None, but bagged mode was specified. If calling from `TabularPredictor.fit()`, `tuning_data` must be None.\u001B[39m\u001B[39m\\n\u001B[39;00m\u001B[39m'\u001B[39m\n\u001B[1;32m     82\u001B[0m                              \u001B[39m'\u001B[39m\u001B[39mBagged mode does not use tuning data / validation data. Instead, all data (`train_data` and `tuning_data`) should be combined and specified as `train_data`.\u001B[39m\u001B[39m\\n\u001B[39;00m\u001B[39m'\u001B[39m\n\u001B[1;32m     83\u001B[0m                              \u001B[39m'\u001B[39m\u001B[39mBagging/Stacking with a held-out validation set (blend stacking) is not yet supported.\u001B[39m\u001B[39m'\u001B[39m)\n\u001B[0;32m---> 85\u001B[0m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_train_multi_and_ensemble(X\u001B[39m=\u001B[39;49mX,\n\u001B[1;32m     86\u001B[0m                                y\u001B[39m=\u001B[39;49my,\n\u001B[1;32m     87\u001B[0m                                X_val\u001B[39m=\u001B[39;49mX_val,\n\u001B[1;32m     88\u001B[0m                                y_val\u001B[39m=\u001B[39;49my_val,\n\u001B[1;32m     89\u001B[0m                                X_unlabeled\u001B[39m=\u001B[39;49mX_unlabeled,\n\u001B[1;32m     90\u001B[0m                                hyperparameters\u001B[39m=\u001B[39;49mhyperparameters,\n\u001B[1;32m     91\u001B[0m                                num_stack_levels\u001B[39m=\u001B[39;49mnum_stack_levels,\n\u001B[1;32m     92\u001B[0m                                time_limit\u001B[39m=\u001B[39;49mtime_limit,\n\u001B[1;32m     93\u001B[0m                                core_kwargs\u001B[39m=\u001B[39;49mcore_kwargs,\n\u001B[1;32m     94\u001B[0m                                infer_limit\u001B[39m=\u001B[39;49minfer_limit,\n\u001B[1;32m     95\u001B[0m                                infer_limit_batch_size\u001B[39m=\u001B[39;49minfer_limit_batch_size,\n\u001B[1;32m     96\u001B[0m                                groups\u001B[39m=\u001B[39;49mgroups)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:1665\u001B[0m, in \u001B[0;36mAbstractTrainer._train_multi_and_ensemble\u001B[0;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001B[0m\n\u001B[1;32m   1663\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_num_rows_train \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m \u001B[39mlen\u001B[39m(X_val)\n\u001B[1;32m   1664\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_num_cols_train \u001B[39m=\u001B[39m \u001B[39mlen\u001B[39m(\u001B[39mlist\u001B[39m(X\u001B[39m.\u001B[39mcolumns))\n\u001B[0;32m-> 1665\u001B[0m model_names_fit \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mtrain_multi_levels(X, y, hyperparameters\u001B[39m=\u001B[39;49mhyperparameters, X_val\u001B[39m=\u001B[39;49mX_val, y_val\u001B[39m=\u001B[39;49my_val,\n\u001B[1;32m   1666\u001B[0m                                           X_unlabeled\u001B[39m=\u001B[39;49mX_unlabeled, level_start\u001B[39m=\u001B[39;49m\u001B[39m1\u001B[39;49m, level_end\u001B[39m=\u001B[39;49mnum_stack_levels\u001B[39m+\u001B[39;49m\u001B[39m1\u001B[39;49m, time_limit\u001B[39m=\u001B[39;49mtime_limit, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1667\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mlen\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mget_model_names()) \u001B[39m==\u001B[39m \u001B[39m0\u001B[39m:\n\u001B[1;32m   1668\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\u001B[39m'\u001B[39m\u001B[39mAutoGluon did not successfully train any models\u001B[39m\u001B[39m'\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:295\u001B[0m, in \u001B[0;36mAbstractTrainer.train_multi_levels\u001B[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001B[0m\n\u001B[1;32m    293\u001B[0m         core_kwargs_level[\u001B[39m'\u001B[39m\u001B[39mtime_limit\u001B[39m\u001B[39m'\u001B[39m] \u001B[39m=\u001B[39m core_kwargs_level\u001B[39m.\u001B[39mget(\u001B[39m'\u001B[39m\u001B[39mtime_limit\u001B[39m\u001B[39m'\u001B[39m, time_limit_core)\n\u001B[1;32m    294\u001B[0m         aux_kwargs_level[\u001B[39m'\u001B[39m\u001B[39mtime_limit\u001B[39m\u001B[39m'\u001B[39m] \u001B[39m=\u001B[39m aux_kwargs_level\u001B[39m.\u001B[39mget(\u001B[39m'\u001B[39m\u001B[39mtime_limit\u001B[39m\u001B[39m'\u001B[39m, time_limit_aux)\n\u001B[0;32m--> 295\u001B[0m     base_model_names, aux_models \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mstack_new_level(\n\u001B[1;32m    296\u001B[0m         X\u001B[39m=\u001B[39;49mX, y\u001B[39m=\u001B[39;49my, X_val\u001B[39m=\u001B[39;49mX_val, y_val\u001B[39m=\u001B[39;49my_val, X_unlabeled\u001B[39m=\u001B[39;49mX_unlabeled,\n\u001B[1;32m    297\u001B[0m         models\u001B[39m=\u001B[39;49mhyperparameters, level\u001B[39m=\u001B[39;49mlevel, base_model_names\u001B[39m=\u001B[39;49mbase_model_names,\n\u001B[1;32m    298\u001B[0m         core_kwargs\u001B[39m=\u001B[39;49mcore_kwargs_level, aux_kwargs\u001B[39m=\u001B[39;49maux_kwargs_level, name_suffix\u001B[39m=\u001B[39;49mname_suffix,\n\u001B[1;32m    299\u001B[0m         infer_limit\u001B[39m=\u001B[39;49minfer_limit, infer_limit_batch_size\u001B[39m=\u001B[39;49minfer_limit_batch_size,\n\u001B[1;32m    300\u001B[0m     )\n\u001B[1;32m    301\u001B[0m     model_names_fit \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m base_model_names \u001B[39m+\u001B[39m aux_models\n\u001B[1;32m    302\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_time_limit \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:405\u001B[0m, in \u001B[0;36mAbstractTrainer.stack_new_level\u001B[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size)\u001B[0m\n\u001B[1;32m    403\u001B[0m     core_kwargs[\u001B[39m'\u001B[39m\u001B[39mname_suffix\u001B[39m\u001B[39m'\u001B[39m] \u001B[39m=\u001B[39m core_kwargs\u001B[39m.\u001B[39mget(\u001B[39m'\u001B[39m\u001B[39mname_suffix\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39m'\u001B[39m) \u001B[39m+\u001B[39m name_suffix\n\u001B[1;32m    404\u001B[0m     aux_kwargs[\u001B[39m'\u001B[39m\u001B[39mname_suffix\u001B[39m\u001B[39m'\u001B[39m] \u001B[39m=\u001B[39m aux_kwargs\u001B[39m.\u001B[39mget(\u001B[39m'\u001B[39m\u001B[39mname_suffix\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39m'\u001B[39m) \u001B[39m+\u001B[39m name_suffix\n\u001B[0;32m--> 405\u001B[0m core_models \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mstack_new_level_core(X\u001B[39m=\u001B[39;49mX, y\u001B[39m=\u001B[39;49my, X_val\u001B[39m=\u001B[39;49mX_val, y_val\u001B[39m=\u001B[39;49my_val, X_unlabeled\u001B[39m=\u001B[39;49mX_unlabeled, models\u001B[39m=\u001B[39;49mmodels,\n\u001B[1;32m    406\u001B[0m                                         level\u001B[39m=\u001B[39;49mlevel, infer_limit\u001B[39m=\u001B[39;49minfer_limit, infer_limit_batch_size\u001B[39m=\u001B[39;49minfer_limit_batch_size, base_model_names\u001B[39m=\u001B[39;49mbase_model_names, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mcore_kwargs)\n\u001B[1;32m    408\u001B[0m \u001B[39mif\u001B[39;00m X_val \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[1;32m    409\u001B[0m     aux_models \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mstack_new_level_aux(X\u001B[39m=\u001B[39mX, y\u001B[39m=\u001B[39my, base_model_names\u001B[39m=\u001B[39mcore_models, level\u001B[39m=\u001B[39mlevel\u001B[39m+\u001B[39m\u001B[39m1\u001B[39m,\n\u001B[1;32m    410\u001B[0m                                           infer_limit\u001B[39m=\u001B[39minfer_limit, infer_limit_batch_size\u001B[39m=\u001B[39minfer_limit_batch_size, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39maux_kwargs)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:496\u001B[0m, in \u001B[0;36mAbstractTrainer.stack_new_level_core\u001B[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001B[0m\n\u001B[1;32m    493\u001B[0m fit_kwargs \u001B[39m=\u001B[39m \u001B[39mdict\u001B[39m(num_classes\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mnum_classes)\n\u001B[1;32m    495\u001B[0m \u001B[39m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001B[39;00m\n\u001B[0;32m--> 496\u001B[0m \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_train_multi(X\u001B[39m=\u001B[39;49mX_init, y\u001B[39m=\u001B[39;49my, X_val\u001B[39m=\u001B[39;49mX_val, y_val\u001B[39m=\u001B[39;49my_val, X_unlabeled\u001B[39m=\u001B[39;49mX_unlabeled,\n\u001B[1;32m    497\u001B[0m                          models\u001B[39m=\u001B[39;49mmodels, level\u001B[39m=\u001B[39;49mlevel, stack_name\u001B[39m=\u001B[39;49mstack_name, compute_score\u001B[39m=\u001B[39;49mcompute_score, fit_kwargs\u001B[39m=\u001B[39;49mfit_kwargs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:1635\u001B[0m, in \u001B[0;36mAbstractTrainer._train_multi\u001B[0;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001B[0m\n\u001B[1;32m   1633\u001B[0m \u001B[39mif\u001B[39;00m n_repeat_start \u001B[39m==\u001B[39m \u001B[39m0\u001B[39m:\n\u001B[1;32m   1634\u001B[0m     time_start \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime()\n\u001B[0;32m-> 1635\u001B[0m     model_names_trained \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_train_multi_initial(X\u001B[39m=\u001B[39;49mX, y\u001B[39m=\u001B[39;49my, models\u001B[39m=\u001B[39;49mmodels, k_fold\u001B[39m=\u001B[39;49mk_fold, n_repeats\u001B[39m=\u001B[39;49mn_repeats_initial, hyperparameter_tune_kwargs\u001B[39m=\u001B[39;49mhyperparameter_tune_kwargs,\n\u001B[1;32m   1636\u001B[0m                                                     feature_prune_kwargs\u001B[39m=\u001B[39;49mfeature_prune_kwargs, time_limit\u001B[39m=\u001B[39;49mtime_limit, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1637\u001B[0m     n_repeat_start \u001B[39m=\u001B[39m n_repeats_initial\n\u001B[1;32m   1638\u001B[0m     \u001B[39mif\u001B[39;00m time_limit \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:1521\u001B[0m, in \u001B[0;36mAbstractTrainer._train_multi_initial\u001B[0;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001B[0m\n\u001B[1;32m   1519\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m bagged:\n\u001B[1;32m   1520\u001B[0m     time_ratio \u001B[39m=\u001B[39m hpo_time_ratio \u001B[39mif\u001B[39;00m hpo_enabled \u001B[39melse\u001B[39;00m \u001B[39m1\u001B[39m\n\u001B[0;32m-> 1521\u001B[0m     models \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_train_multi_fold(models\u001B[39m=\u001B[39;49mmodels, hyperparameter_tune_kwargs\u001B[39m=\u001B[39;49mhyperparameter_tune_kwargs,\n\u001B[1;32m   1522\u001B[0m                                     time_limit\u001B[39m=\u001B[39;49mtime_limit, time_split\u001B[39m=\u001B[39;49mtime_split, time_ratio\u001B[39m=\u001B[39;49mtime_ratio, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mfit_args)\n\u001B[1;32m   1523\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m   1524\u001B[0m     bagged_time_start \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime()\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:1606\u001B[0m, in \u001B[0;36mAbstractTrainer._train_multi_fold\u001B[0;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001B[0m\n\u001B[1;32m   1604\u001B[0m         time_start_model \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime()\n\u001B[1;32m   1605\u001B[0m         time_left \u001B[39m=\u001B[39m time_limit \u001B[39m-\u001B[39m (time_start_model \u001B[39m-\u001B[39m time_start)\n\u001B[0;32m-> 1606\u001B[0m model_name_trained_lst \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_train_single_full(X, y, model, time_limit\u001B[39m=\u001B[39;49mtime_left, hyperparameter_tune_kwargs\u001B[39m=\u001B[39;49mhyperparameter_tune_kwargs_model, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1608\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mlow_memory:\n\u001B[1;32m   1609\u001B[0m     \u001B[39mdel\u001B[39;00m model\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:1407\u001B[0m, in \u001B[0;36mAbstractTrainer._train_single_full\u001B[0;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, **kwargs)\u001B[0m\n\u001B[1;32m   1405\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_extra_banned_names\u001B[39m.\u001B[39madd(model\u001B[39m.\u001B[39mname)\n\u001B[1;32m   1406\u001B[0m \u001B[39mfor\u001B[39;00m model_hpo_name, model_info \u001B[39min\u001B[39;00m hpo_models\u001B[39m.\u001B[39mitems():\n\u001B[0;32m-> 1407\u001B[0m     model_hpo \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mload_model(model_hpo_name, path\u001B[39m=\u001B[39mmodel_info[\u001B[39m'\u001B[39;49m\u001B[39mpath\u001B[39;49m\u001B[39m'\u001B[39;49m], model_type\u001B[39m=\u001B[39m\u001B[39mtype\u001B[39m(model))\n\u001B[1;32m   1408\u001B[0m     logger\u001B[39m.\u001B[39mlog(\u001B[39m20\u001B[39m, \u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39mFitted model: \u001B[39m\u001B[39m{\u001B[39;00mmodel_hpo\u001B[39m.\u001B[39mname\u001B[39m}\u001B[39;00m\u001B[39m ...\u001B[39m\u001B[39m'\u001B[39m)\n\u001B[1;32m   1409\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_add_model(model\u001B[39m=\u001B[39mmodel_hpo, stack_name\u001B[39m=\u001B[39mstack_name, level\u001B[39m=\u001B[39mlevel):\n",
      "\u001B[0;31mTypeError\u001B[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "metric_reg = 'mean_absolute_error'\n",
    "label_reg = 'price_per_sqft'\n",
    "train_data_reg = TabularDataset(X_train) \n",
    "valid_data_reg = TabularDataset(X_valid)\n",
    "test_data_reg = TabularDataset(df_test)\n",
    "#train_data_reg = train_data_reg.drop(['price', 'size_sqft'], axis=1)\n",
    "test_data_reg = test_data_reg.drop(['size_sqft'], axis=1)\n",
    "\n",
    "predictor = TabularPredictor(label = label_reg, eval_metric = metric_reg)\n",
    "\n",
    "gbm_options = {  # specifies non-default hyperparameter values for lightGBM gradient boosted trees\n",
    "    'num_boost_round': 100,  # number of boosting rounds (controls training time of GBM models)\n",
    "    'num_leaves': ag.space.Int(lower=26, upper=66, default=36),  # number of leaves in trees (integer hyperparameter)\n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    " 'GBM': gbm_options,\n",
    " 'XGB': [{'booster':'gblinear','alpha':1, 'lambda':1 },\n",
    "    {'booster':'gbtree','alpha':0.1, 'lambda':1 },\n",
    "    {'booster':'gblinear','alpha':0.5, 'lambda':1 },\n",
    "    {'booster':'gblinear','alpha':0.7, 'lambda':1 },\n",
    "    {'booster':'gbtree','alpha':0.1, 'lambda':1 },\n",
    "    {'booster':'gbtree','alpha':0.5, 'lambda':1 },\n",
    "    {'alpha':1}],\n",
    "'RF': [{'criterion': 'gini',\n",
    "   'ag_args': {'name_suffix': 'Gini',\n",
    "    'problem_types': ['binary', 'multiclass']}},\n",
    "  {'criterion': 'entropy',\n",
    "   'ag_args': {'name_suffix': 'Entr',\n",
    "    'problem_types': ['binary', 'multiclass']}},\n",
    "  {'criterion': 'squared_error',\n",
    "   'ag_args': {'name_suffix': 'MSE',\n",
    "    'problem_types': ['regression', 'quantile']}}],\n",
    " 'XT': [{'criterion': 'gini',\n",
    "   'ag_args': {'name_suffix': 'Gini',\n",
    "    'problem_types': ['binary', 'multiclass']}},\n",
    "  {'criterion': 'entropy',\n",
    "   'ag_args': {'name_suffix': 'Entr',\n",
    "    'problem_types': ['regression']}},\n",
    "  {'criterion': 'squared_error',\n",
    "   'ag_args': {'name_suffix': 'MSE',\n",
    "    'problem_types': ['regression', 'quantile']}}]\n",
    "}\n",
    "\n",
    "time_limit = 2*60  # train various models for ~2 min\n",
    "num_trials = 5  # try at most 5 different hyperparameter configurations for each type of model\n",
    "search_strategy = 'auto'  # to tune hyperparameters using random search routine with a local scheduler\n",
    "\n",
    "hyperparameter_tune_kwargs = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "    'num_trials': num_trials,\n",
    "    'scheduler' : 'local',\n",
    "    'searcher': search_strategy,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "excluded_model_types = ['NN_TORCH', 'FASTAI', 'KNN', 'CAT', 'RF']\n",
    "predictor_reg = predictor.fit(train_data_reg, excluded_model_types = excluded_model_types, time_limit = 900\n",
    ",hyperparameters=hyperparameters\n",
    ",tuning_data=valid_data_reg, hyperparameter_tune_kwargs=hyperparameter_tune_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/Users/nishita/Documents/Semester 1/CS5228/project/notebooks/AutogluonModels/ag-20221019_034135\n",
    "folderName = \"AutogluonModels/ag-20221019_034135/\"\n",
    "predictor = TabularPredictor.load(folderName)\n",
    "\n",
    "predictor.fit_hyperparameters_\n",
    "##test_data_reg = TabularDataset(df_test)\n",
    "#test_data_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderName = \"AutogluonModels/ag-20221019_103622/\"\n",
    "#With target encoding and less one hot encoding.\n",
    "predictor = TabularPredictor.load(folderName)\n",
    "results = predictor.fit_summary(show_plot=True)\n",
    "\n",
    "def writePredictionsToFile(modelName, folder = folderName, refit = False):\n",
    "    if(refit == True):\n",
    "        modelName = modelName +  '_FULL'    \n",
    "    y_pred = predictor.predict(test_data_reg, model=modelName)\n",
    "    yTest = y_pred * df_test_size_sqft\n",
    "    yTest = pd.DataFrame(yTest, columns=['Predicted'], index=df_test.index)\n",
    "    yTest.to_csv(f'{folder}/predictions/{modelName}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writePredictionsToFile('WeightedEnsemble_L2')\n",
    "writePredictionsToFile('RandomForestMSE')\n",
    "writePredictionsToFile('LightGBMLarge')\n",
    "#writePredictionsToFile('CatBoost')\n",
    "writePredictionsToFile('XGBoost')\n",
    "writePredictionsToFile('ExtraTreesMSE')\n",
    "writePredictionsToFile('LightGBM')\n",
    "writePredictionsToFile('LightGBMXT')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.refit_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}